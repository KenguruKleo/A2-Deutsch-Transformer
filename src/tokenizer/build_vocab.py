"""
build_vocab.py ‚Äî –°—Ç–≤–æ—Ä—é—î vocab.json –∑—ñ —Å–ª–æ–≤–Ω–∏–∫–æ–º ~2000 —Ç–æ–∫–µ–Ω—ñ–≤ –¥–ª—è A2 German Grammar Tutor.

–ó–∞–ø—É—Å–∫:
    python build_vocab.py          # ‚Üí —Å—Ç–≤–æ—Ä–∏—Ç—å vocab.json
    python build_vocab.py --stats  # ‚Üí –ø–æ–∫–∞–∂–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è—Ö

–°–ª–æ–≤–Ω–∏–∫ –æ—Ä–≥–∞–Ω—ñ–∑–æ–≤–∞–Ω–∏–π –∑–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è–º–∏:
    1. –°–ø–µ—Ü—ñ–∞–ª—å–Ω—ñ —Ç–æ–∫–µ–Ω–∏ (<PAD>, <BOS>, <EOS>, <UNK>)
    2. –ü—É–Ω–∫—Ç—É–∞—Ü—ñ—è —Ç–∞ –º–∞—Ä–∫–µ—Ä–∏
    3. –ê—Ä—Ç–∏–∫–ª—ñ —Ç–∞ –≤–∫–∞–∑—ñ–≤–Ω—ñ —Å–ª–æ–≤–∞
    4. –ó–∞–π–º–µ–Ω–Ω–∏–∫–∏ (–æ—Å–æ–±–æ–≤—ñ, –ø—Ä–∏—Å–≤—ñ–π–Ω—ñ, –∑–≤–æ—Ä–æ—Ç–Ω—ñ)
    5. –ü—Ä–∏–π–º–µ–Ω–Ω–∏–∫–∏
    6. –°–ø–æ–ª—É—á–Ω–∏–∫–∏ —Ç–∞ –ø—Ä–∏—Å–ª—ñ–≤–Ω–∏–∫–∏
    7. –ú–æ–¥–∞–ª—å–Ω—ñ –¥—ñ—î—Å–ª–æ–≤–∞ –∑ —É—Å—ñ–º–∞ —Ñ–æ—Ä–º–∞–º–∏
    8. –û—Å–Ω–æ–≤–Ω—ñ –¥—ñ—î—Å–ª–æ–≤–∞ –∑ —Ñ–æ—Ä–º–∞–º–∏ (Pr√§sens, Perfekt, Partizip II)
    9. –í—ñ–¥–æ–∫—Ä–µ–º–ª—é–≤–∞–Ω—ñ –¥—ñ—î—Å–ª–æ–≤–∞ (Separable verbs)
    10. –Ü–º–µ–Ω–Ω–∏–∫–∏ (A2 —Ç–µ–º–∏: —Å—ñ–º'—è, —ó–∂–∞, –ø–æ–±—É—Ç, –º—ñ—Å—Ç–æ, —Ä–æ–±–æ—Ç–∞‚Ä¶)
    11. –ü—Ä–∏–∫–º–µ—Ç–Ω–∏–∫–∏
    12. –ß–∏—Å–ª—ñ–≤–Ω–∏–∫–∏
    13. –°–ª—É–∂–±–æ–≤—ñ/—á–∞—Å—Ç–æ—Ç–Ω—ñ —Å–ª–æ–≤–∞
    14. –°–ª–æ–≤–∞ –¥–ª—è –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π —Ç—å—é—Ç–æ—Ä–∞ (Correct, Incorrect, Explanation‚Ä¶)
"""

import json
import sys
from pathlib import Path

# Add project root to sys.path
sys.path.append(str(Path(__file__).parent.parent.parent))
from src.config import load_config

def build_vocab() -> dict[str, int]:
    """–ó–±–∏—Ä–∞—î –≤—Å–µ –≤ –æ–¥–∏–Ω –≤–µ–ª–∏–∫–∏–π —Å–ª–æ–≤–Ω–∏–∫: token ‚Üí id."""
    tokens: list[str] = []

    # ‚îÄ‚îÄ‚îÄ 1. –°–ø–µ—Ü—ñ–∞–ª—å–Ω—ñ —Ç–æ–∫–µ–Ω–∏ (4) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # PAD ‚Äî –∑–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ä–µ—á–µ–Ω—å –¥–æ –æ–¥–Ω–∞–∫–æ–≤–æ—ó –¥–æ–≤–∂–∏–Ω–∏
    # BOS ‚Äî "beginning of sequence" ‚Äî –ø–æ—á–∞—Ç–æ–∫ —Ç–µ–∫—Å—Ç—É
    # EOS ‚Äî "end of sequence" ‚Äî –∫—ñ–Ω–µ—Ü—å —Ç–µ–∫—Å—Ç—É
    # UNK ‚Äî "unknown" ‚Äî –Ω–µ–≤—ñ–¥–æ–º–µ —Å–ª–æ–≤–æ
    special = ["<PAD>", "<BOS>", "<EOS>", "<UNK>"]
    tokens.extend(special)

    # ‚îÄ‚îÄ‚îÄ 2. –ü—É–Ω–∫—Ç—É–∞—Ü—ñ—è —Ç–∞ –º–∞—Ä–∫–µ—Ä–∏ (25) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    punctuation = [
        ".", ",", "!", "?", ":", ";", "-", "(", ")", '"', "'",
        "...",
        # –ú–∞—Ä–∫–µ—Ä–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ —Ç—å—é—Ç–æ—Ä–∞
        "‚úÖ", "‚ùå", "üìù",
        # –ù–æ–≤—ñ —Ä—è–¥–∫–∏ —ñ –ø—Ä–æ–±—ñ–ª–∏ (—è–∫ –æ–∫—Ä–µ–º—ñ —Ç–æ–∫–µ–Ω–∏)
        "\n",
        # Emoji-like –º–∞—Ä–∫–µ—Ä–∏, —è–∫—ñ –º–æ–¥–µ–ª—å –±–∞—á–∏—Ç–∏–º–µ —É –≤—ñ–¥–ø–æ–≤—ñ–¥—è—Ö
        "Correct", "Incorrect", "Correct:", "Incorrect.",
        "Explanation", "Explanation:",
        # C–ø–µ—Ü—ñ–∞–ª—å–Ω—ñ —Å–ª–æ–≤–∞ –¥–ª—è —Ñ–æ—Ä–º–∞—Ç—É –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ
        "correct:", "incorrect.",
    ]
    tokens.extend(punctuation)

    # ‚îÄ‚îÄ‚îÄ 3. –ê—Ä—Ç–∏–∫–ª—ñ —Ç–∞ –≤–∫–∞–∑—ñ–≤–Ω—ñ/–Ω–µ–æ–∑–Ω–∞—á–µ–Ω—ñ (30) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    articles = [
        # –û–∑–Ω–∞—á–µ–Ω—ñ –∞—Ä—Ç–∏–∫–ª—ñ (Nominativ, Akkusativ, Dativ)
        "der", "die", "das", "den", "dem", "des",
        # –ù–µ–æ–∑–Ω–∞—á–µ–Ω—ñ –∞—Ä—Ç–∏–∫–ª—ñ
        "ein", "eine", "einen", "einem", "einer",
        # –ó–∞–ø–µ—Ä–µ—á–Ω—ñ –∞—Ä—Ç–∏–∫–ª—ñ
        "kein", "keine", "keinen", "keinem", "keiner", "keines",
        # –í–∫–∞–∑—ñ–≤–Ω—ñ
        "dieser", "diese", "dieses", "diesen", "diesem",
        # jeder
        "jeder", "jede", "jedes", "jeden", "jedem",
    ]
    tokens.extend(articles)

    # ‚îÄ‚îÄ‚îÄ 4. –ó–∞–π–º–µ–Ω–Ω–∏–∫–∏ (55) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    pronouns = [
        # –û—Å–æ–±–æ–≤—ñ (Nominativ)
        "ich", "du", "er", "sie", "es", "wir", "ihr",
        # –û—Å–æ–±–æ–≤—ñ (Akkusativ)
        "mich", "dich", "ihn", "uns", "euch",
        # –û—Å–æ–±–æ–≤—ñ (Dativ)
        "mir", "dir", "ihm", "ihr",  # "ihr" –≤–∂–µ —î, –¥–æ–¥–∞–º–æ "ihnen"
        "ihnen",
        # –ü—Ä–∏—Å–≤—ñ–π–Ω—ñ (Nominativ ‚Äî –æ—Å–Ω–æ–≤–Ω—ñ —Ñ–æ—Ä–º–∏)
        "mein", "meine", "meinen", "meinem", "meiner",
        "dein", "deine", "deinen", "deinem", "deiner",
        "sein", "seine", "seinen", "seinem", "seiner",
        "ihre", "ihren", "ihrem", "ihrer",  # ihr/ihre
        "unser", "unsere", "unseren", "unserem",
        "euer", "eure", "euren", "eurem",
        # –ó–≤–æ—Ä–æ—Ç–Ω—ñ
        "sich",
        # –í—ñ–¥–Ω–æ—Å–Ω—ñ / –ø–∏—Ç–∞–ª—å–Ω—ñ
        "wer", "was", "wen", "wem",
        "man",
    ]
    tokens.extend(pronouns)

    # ‚îÄ‚îÄ‚îÄ 5. –ü—Ä–∏–π–º–µ–Ω–Ω–∏–∫–∏ (35) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    prepositions = [
        # + Dativ
        "mit", "nach", "bei", "von", "zu", "aus", "seit", "gegen√ºber",
        # + Akkusativ
        "f√ºr", "ohne", "gegen", "durch", "um", "bis",
        # Wechselpr√§positionen (+ Dat –∞–±–æ + Akk)
        "in", "an", "auf", "√ºber", "unter", "vor", "hinter",
        "neben", "zwischen",
        # –°–∫–æ—Ä–æ—á–µ–Ω—ñ —Ñ–æ—Ä–º–∏
        "im", "am", "zum", "zur", "ins", "ans", "vom",
        # –î–æ–¥–∞—Ç–∫–æ–≤—ñ
        "ab", "au√üer",
    ]
    tokens.extend(prepositions)

    # ‚îÄ‚îÄ‚îÄ 6. –°–ø–æ–ª—É—á–Ω–∏–∫–∏ —Ç–∞ –ø—Ä–∏—Å–ª—ñ–≤–Ω–∏–∫–∏ (75) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    conjunctions_adverbs = [
        # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ü—ñ–π–Ω—ñ —Å–ø–æ–ª—É—á–Ω–∏–∫–∏
        "und", "oder", "aber", "denn", "sondern",
        # –ü—ñ–¥—Ä—è–¥–Ω—ñ —Å–ø–æ–ª—É—á–Ω–∏–∫–∏ (–ø–æ—Ä—è–¥–æ–∫ —Å–ª—ñ–≤!)
        "weil", "dass", "wenn", "ob", "als", "obwohl",
        "damit", "bevor", "nachdem", "w√§hrend",
        # –ü—Ä–∏—Å–ª—ñ–≤–Ω–∏–∫–∏ —á–∞—Å—É
        "heute", "morgen", "gestern", "jetzt", "dann", "danach",
        "immer", "nie", "oft", "manchmal", "selten",
        "schon", "noch", "bald", "gerade", "fr√ºher", "sp√§ter",
        "montags", "dienstags", "mittwochs", "donnerstags",
        "freitags", "samstags", "sonntags",
        # –ü—Ä–∏—Å–ª—ñ–≤–Ω–∏–∫–∏ –º—ñ—Å—Ü—è
        "hier", "dort", "da", "oben", "unten", "links", "rechts",
        "drau√üen", "drinnen", "√ºberall",
        # –ü—Ä–∏—Å–ª—ñ–≤–Ω–∏–∫–∏ —Å–ø–æ—Å–æ–±—É
        "sehr", "gern", "gerne", "lieber", "viel", "wenig",
        "schnell", "langsam", "zusammen", "allein",
        "ungef√§hr", "etwa", "fast", "genug",
        # –ü–∏—Ç–∞–ª—å–Ω—ñ —Å–ª–æ–≤–∞
        "wie", "wo", "wann", "warum", "wohin", "woher",
    ]
    tokens.extend(conjunctions_adverbs)

    # ‚îÄ‚îÄ‚îÄ 7. –ú–æ–¥–∞–ª—å–Ω—ñ –¥—ñ—î—Å–ª–æ–≤–∞ –∑ —É—Å—ñ–º–∞ —Ñ–æ—Ä–º–∞–º–∏ (36) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # –ö–æ–∂–Ω–µ –º–æ–¥–∞–ª—å–Ω–µ –¥—ñ—î—Å–ª–æ–≤–æ –º–∞—î 6 —Ñ–æ—Ä–º Pr√§sens + Pr√§teritum
    modal_verbs = [
        # k√∂nnen
        "k√∂nnen", "kann", "kannst", "k√∂nnt", "konnte", "konnten",
        # m√ºssen
        "m√ºssen", "muss", "musst", "m√ºsst", "musste", "mussten",
        # d√ºrfen
        "d√ºrfen", "darf", "darfst", "d√ºrft", "durfte", "durften",
        # wollen
        "wollen", "will", "willst", "wollt", "wollte", "wollten",
        # sollen
        "sollen", "soll", "sollst", "sollt", "sollte", "sollten",
        # m√∂gen / m√∂chten
        "m√∂gen", "mag", "magst", "m√∂chten", "m√∂chte", "m√∂chtest",
    ]
    tokens.extend(modal_verbs)

    # ‚îÄ‚îÄ‚îÄ 8. –û—Å–Ω–æ–≤–Ω—ñ –¥—ñ—î—Å–ª–æ–≤–∞ –∑ —Ñ–æ—Ä–º–∞–º–∏ (350) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # –§–æ—Ä–º–∞—Ç: —ñ–Ω—Ñ—ñ–Ω—ñ—Ç–∏–≤, ich, du, er/sie/es, wir/sie, Partizip II
    # –î–æ–ø–æ–º—ñ–∂–Ω—ñ –¥—ñ—î—Å–ª–æ–≤–∞
    core_verbs = [
        # sein (to be) ‚Äî –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–µ!
        "bin", "bist", "ist", "sind", "seid",
        "war", "warst", "waren", "wart",
        "gewesen",
        # –Ü–Ω—Ñ—ñ–Ω—ñ—Ç–∏–≤ "sein" –¥–æ–¥–∞–º–æ –æ–∫—Ä–µ–º–æ
        "sein",
        # haben (to have)
        "haben", "habe", "hast", "hat", "habt",
        "hatte", "hattest", "hatten", "hattet",
        "gehabt",
        # werden (to become / auxiliary)
        "werden", "werde", "wirst", "wird", "werdet",
        "wurde", "geworden",
    ]
    tokens.extend(core_verbs)

    # A2 –¥—ñ—î—Å–ª–æ–≤–∞ (—ñ–Ω—Ñ—ñ–Ω—ñ—Ç–∏–≤ + –∫–ª—é—á–æ–≤—ñ —Ñ–æ—Ä–º–∏ + Partizip II)
    a2_verbs = [
        # –†—É—Ö / –ø–µ—Ä–µ–º—ñ—â–µ–Ω–Ω—è (–∑ sein!)
        "gehen", "gehe", "gehst", "geht", "gegangen",
        "kommen", "komme", "kommst", "kommt", "gekommen",
        "fahren", "fahre", "f√§hrst", "f√§hrt", "gefahren",
        "laufen", "laufe", "l√§ufst", "l√§uft", "gelaufen",
        "fliegen", "fliege", "fliegst", "fliegt", "geflogen",
        "schwimmen", "schwimme", "schwimmst", "schwimmt", "geschwommen",
        "bleiben", "bleibe", "bleibst", "bleibt", "geblieben",
        "reisen", "reise", "reist",  "gereist",

        # –ü–æ–≤—Å—è–∫–¥–µ–Ω–Ω–µ –∂–∏—Ç—Ç—è
        "machen", "mache", "machst", "macht", "gemacht",
        "arbeiten", "arbeite", "arbeitest", "arbeitet", "gearbeitet",
        "lernen", "lerne", "lernst", "lernt", "gelernt",
        "spielen", "spiele", "spielst", "spielt", "gespielt",
        "kaufen", "kaufe", "kaufst", "kauft", "gekauft",
        "kochen", "koche", "kochst", "kocht", "gekocht",
        "essen", "esse", "isst", "gegessen",
        "trinken", "trinke", "trinkst", "trinkt", "getrunken",
        "schlafen", "schlafe", "schl√§fst", "schl√§ft", "geschlafen",
        "wohnen", "wohne", "wohnst", "wohnt", "gewohnt",
        "leben", "lebe", "lebst", "lebt", "gelebt",

        # –ö–æ–º—É–Ω—ñ–∫–∞—Ü—ñ—è
        "sprechen", "spreche", "sprichst", "spricht", "gesprochen",
        "sagen", "sage", "sagst", "sagt", "gesagt",
        "fragen", "frage", "fragst", "fragt", "gefragt",
        "antworten", "antworte", "antwortest", "antwortet", "geantwortet",
        "erz√§hlen", "erz√§hle", "erz√§hlst", "erz√§hlt",
        "schreiben", "schreibe", "schreibst", "schreibt", "geschrieben",
        "lesen", "lese", "liest", "gelesen",
        "h√∂ren", "h√∂re", "h√∂rst", "h√∂rt", "geh√∂rt",
        "verstehen", "verstehe", "verstehst", "versteht", "verstanden",
        "helfen", "helfe", "hilfst", "hilft", "geholfen",
        "rufen", "rufe", "rufst", "ruft", "gerufen",

        # –Ü–Ω—à—ñ —á–∞—Å—Ç—ñ –¥—ñ—î—Å–ª–æ–≤–∞
        "sehen", "sehe", "siehst", "sieht", "gesehen",
        "geben", "gebe", "gibst", "gibt", "gegeben",
        "nehmen", "nehme", "nimmst", "nimmt", "genommen",
        "finden", "finde", "findest", "findet", "gefunden",
        "wissen", "wei√ü", "wei√üt", "gewusst",
        "denken", "denke", "denkst", "denkt", "gedacht",
        "glauben", "glaube", "glaubst", "glaubt", "geglaubt",
        "brauchen", "brauche", "brauchst", "braucht", "gebraucht",
        "bringen", "bringe", "bringst", "bringt", "gebracht",
        "legen", "lege", "legst", "legt", "gelegt",
        "stellen", "stelle", "stellst", "stellt", "gestellt",
        "setzen", "setze", "setzt", "gesetzt",
        "liegen", "liege", "liegst", "liegt", "gelegen",
        "stehen", "stehe", "stehst", "steht", "gestanden",
        "sitzen", "sitze", "sitzt", "gesessen",
        "tragen", "trage", "tr√§gst", "tr√§gt", "getragen",
        "waschen", "wasche", "w√§schst", "w√§scht", "gewaschen",
        "putzen", "putze", "putzt", "geputzt",
        "√∂ffnen", "√∂ffne", "√∂ffnest", "√∂ffnet", "ge√∂ffnet",
        "schlie√üen", "schlie√üe", "schlie√üt", "geschlossen",
        "beginnen", "beginne", "beginnst", "beginnt", "begonnen",
        "besuchen", "besuche", "besuchst", "besucht",
        "bezahlen", "bezahle", "bezahlst", "bezahlt",
        "vergessen", "vergesse", "vergisst", "vergessen",
        "bekommen", "bekomme", "bekommst", "bekommt", "bekommen",
        "treffen", "treffe", "triffst", "trifft", "getroffen",
        "kennen", "kenne", "kennst", "kennt", "gekannt",
        "m√∂gen", "gefallen", "gef√§llt",
        "freuen", "freue", "freust", "freut", "gefreut",
        "hoffen", "hoffe", "hoffst", "hofft", "gehofft",
        "w√ºnschen", "w√ºnsche", "w√ºnschst", "w√ºnscht", "gew√ºnscht",
        "dauern", "dauert",
        "kosten", "kostet",
        "geh√∂ren", "geh√∂rt",
        "passen", "passt",
        "fehlen", "fehlt",
        "stimmen", "stimmt",
        "√§ndern", "√§ndere", "√§nderst", "√§ndert", "ge√§ndert",
        "zeigen", "zeige", "zeigst", "zeigt", "gezeigt",
    ]
    tokens.extend(a2_verbs)

    # ‚îÄ‚îÄ‚îÄ 9. –í—ñ–¥–æ–∫—Ä–µ–º–ª—é–≤–∞–Ω—ñ –¥—ñ—î—Å–ª–æ–≤–∞ (Separable verbs) (80) ‚îÄ
    # –í–∞–∂–ª–∏–≤–æ –¥–ª—è A2: –ø–æ–∑–∏—Ü—ñ—è –ø—Ä–∏—Å—Ç–∞–≤–∫–∏ –∑–º—ñ–Ω—é—î—Ç—å—Å—è!
    # "Ich stehe um 7 Uhr auf." vs "Ich muss aufstehen."
    separable_verbs = [
        # aufstehen
        "aufstehen", "aufgestanden",
        "auf",
        # einkaufen
        "einkaufen", "eingekauft",
        "ein",
        # anfangen
        "anfangen", "angefangen",
        "an",  # –≤–∂–µ —î —è–∫ –ø—Ä–∏–π–º–µ–Ω–Ω–∏–∫, –Ω–µ –¥—É–±–ª—é—î–º–æ
        "fangen", "f√§ngt",
        # aufr√§umen
        "aufr√§umen", "aufger√§umt",
        "r√§ume", "r√§umst", "r√§umt",
        # anrufen
        "anrufen", "angerufen",
        # mitkommen
        "mitkommen", "mitgekommen",
        # mitbringen
        "mitbringen", "mitgebracht",
        # ausgehen
        "ausgehen", "ausgegangen",
        # fernsehen
        "fernsehen", "ferngesehen",
        "fern",
        # zumachen / aufmachen
        "zumachen", "zugemacht",
        "aufmachen", "aufgemacht",
        # ankommen
        "ankommen", "angekommen",
        # abfahren
        "abfahren", "abgefahren",
        # umsteigen
        "umsteigen", "umgestiegen",
        "steige", "steigst", "steigt",
        # einladen
        "einladen", "eingeladen",
        "lade", "l√§dst", "l√§dt",
        # zur√ºckkommen
        "zur√ºckkommen", "zur√ºckgekommen",
        "zur√ºck",
        # vorstellen
        "vorstellen", "vorgestellt",
        "vor",
        # aufh√∂ren
        "aufh√∂ren", "aufgeh√∂rt",
        # weitergehen
        "weitergehen",
        "weiter",
        # vorbereiten
        "vorbereiten", "vorbereitet",
        # teilnehmen
        "teilnehmen", "teilgenommen",
        "teil",
        # –ó–∞–≥–∞–ª—å–Ω—ñ –ø—Ä–∏—Å—Ç–∞–≤–∫–∏ (—è–∫ –æ–∫—Ä–µ–º—ñ —Ç–æ–∫–µ–Ω–∏)
        "ab", "mit", "um", "zu",
    ]
    tokens.extend(separable_verbs)

    # ‚îÄ‚îÄ‚îÄ 10. –Ü–º–µ–Ω–Ω–∏–∫–∏ –ø–æ —Ç–µ–º–∞—Ö A2 (450) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    # –°—ñ–º'—è —Ç–∞ –ª—é–¥–∏
    nouns_family = [
        "Vater", "Mutter", "Eltern", "Kind", "Kinder",
        "Sohn", "Tochter", "Bruder", "Schwester",
        "Gro√üvater", "Gro√ümutter", "Opa", "Oma",
        "Onkel", "Tante", "Cousin", "Cousine",
        "Mann", "Frau", "Freund", "Freundin",
        "Nachbar", "Nachbarin", "Mensch", "Menschen",
        "Leute", "Person", "Junge", "M√§dchen",
        "Baby", "Kollege", "Kollegin", "Chef", "Chefin",
    ]
    tokens.extend(nouns_family)

    # –á–∂–∞ —Ç–∞ –Ω–∞–ø–æ—ó
    nouns_food = [
        "Essen", "Brot", "Br√∂tchen", "Butter", "K√§se",
        "Wurst", "Fleisch", "Fisch", "Ei", "Eier",
        "Reis", "Nudeln", "Kartoffel", "Kartoffeln",
        "Suppe", "Salat", "Gem√ºse", "Obst",
        "Apfel", "Banane", "Orange", "Tomate", "Tomaten",
        "Kuchen", "Schokolade", "Eis",
        "Wasser", "Milch", "Kaffee", "Tee", "Saft",
        "Bier", "Wein", "Getr√§nk",
        "Fr√ºhst√ºck", "Mittagessen", "Abendessen",
        "Mahlzeit", "Zucker", "Salz",
    ]
    tokens.extend(nouns_food)

    # –î—ñ–º —Ç–∞ –ø–æ–±—É—Ç
    nouns_home = [
        "Haus", "Hause", "Wohnung", "Zimmer", "K√ºche",
        "Bad", "Badezimmer", "Schlafzimmer", "Wohnzimmer",
        "Garten", "Balkon", "T√ºr", "Fenster", "Treppe",
        "M√∂bel", "Tisch", "Stuhl", "Bett", "Schrank",
        "Sofa", "Lampe", "Spiegel", "Regal",
        "Fernseher", "Computer", "Handy", "Telefon",
    ]
    tokens.extend(nouns_home)

    # –ú—ñ—Å—Ç–æ —Ç–∞ —Ç—Ä–∞–Ω—Å–ø–æ—Ä—Ç
    nouns_city = [
        "Stadt", "Stra√üe", "Platz", "Park", "Bahnhof",
        "Haltestelle", "Flughafen", "Hotel",
        "Restaurant", "Caf√©", "Laden", "Gesch√§ft",
        "Supermarkt", "Markt", "Apotheke", "Bank",
        "Post", "Kirche", "Museum", "Kino", "Theater",
        "Bibliothek", "Krankenhaus", "Arzt", "√Ñrztin",
        "Polizei", "Schule", "Universit√§t",
        "Bus", "Zug", "U-Bahn", "Stra√üenbahn", "Fahrrad",
        "Auto", "Taxi", "Flugzeug", "Schiff",
        "Ticket", "Fahrkarte", "Weg", "Br√ºcke",
    ]
    tokens.extend(nouns_city)

    # –†–æ–±–æ—Ç–∞ —Ç–∞ –Ω–∞–≤—á–∞–Ω–Ω—è
    nouns_work = [
        "Arbeit", "Beruf", "Job", "B√ºro", "Firma",
        "Lehrer", "Lehrerin", "Sch√ºler", "Sch√ºlerin",
        "Student", "Studentin", "Kurs", "Unterricht",
        "Pr√ºfung", "Aufgabe", "Hausaufgabe", "√úbung",
        "Buch", "Heft", "Stift", "Tafel",
        "Sprache", "Deutsch", "Englisch", "Wort", "Satz",
        "Text", "Brief", "E-Mail", "Nachricht",
        "Zeitung", "Zeitschrift", "Seite",
    ]
    tokens.extend(nouns_work)

    # –ß–∞—Å —Ç–∞ –¥–∞—Ç–∏
    nouns_time = [
        "Zeit", "Uhr", "Stunde", "Minute", "Sekunde",
        "Tag", "Tage", "Woche", "Monat", "Jahr", "Jahre",
        "Morgen", "Mittag", "Abend", "Nacht",
        "Montag", "Dienstag", "Mittwoch", "Donnerstag",
        "Freitag", "Samstag", "Sonntag", "Wochenende",
        "Januar", "Februar", "M√§rz", "April", "Mai", "Juni",
        "Juli", "August", "September", "Oktober", "November", "Dezember",
        "Fr√ºhling", "Sommer", "Herbst", "Winter",
        "Geburtstag", "Feiertag", "Urlaub", "Ferien",
        "Termin", "Datum",
    ]
    tokens.extend(nouns_time)

    # –¢—ñ–ª–æ —Ç–∞ –∑–¥–æ—Ä–æ–≤'—è
    nouns_health = [
        "Kopf", "Auge", "Augen", "Ohr", "Ohren",
        "Nase", "Mund", "Zahn", "Z√§hne",
        "Hand", "H√§nde", "Arm", "Bein", "Fu√ü", "F√º√üe",
        "R√ºcken", "Bauch", "Herz",
        "Gesundheit", "Krankheit", "Schmerzen",
        "Medikament", "Rezept", "Fieber",
    ]
    tokens.extend(nouns_health)

    # –û–¥—è–≥
    nouns_clothes = [
        "Kleidung", "Hemd", "Hose", "Rock", "Kleid",
        "Jacke", "Mantel", "Pullover", "T-Shirt",
        "Schuh", "Schuhe", "Socke", "Socken",
        "M√ºtze", "Tasche", "Koffer",
    ]
    tokens.extend(nouns_clothes)

    # –ü—Ä–∏—Ä–æ–¥–∞ —Ç–∞ –ø–æ–≥–æ–¥–∞
    nouns_nature = [
        "Wetter", "Sonne", "Regen", "Schnee", "Wind",
        "Wolke", "Himmel", "Temperatur", "Grad",
        "Berg", "See", "Meer", "Fluss", "Wald",
        "Baum", "Blume", "Tier", "Hund", "Katze", "Vogel",
    ]
    tokens.extend(nouns_nature)

    # –ê–±—Å—Ç—Ä–∞–∫—Ç–Ω—ñ —Ç–∞ —Ä—ñ–∑–Ω—ñ
    nouns_abstract = [
        "Name", "Adresse", "Nummer", "Alter",
        "Problem", "Frage", "Antwort", "Idee",
        "Hilfe", "Beispiel", "Grund", "Meinung",
        "Erfahrung", "M√∂glichkeit", "Unterschied",
        "Anfang", "Ende", "Ziel", "Plan",
        "Geld", "Preis", "Euro", "Cent",
        "Spa√ü", "Freude", "Angst", "Gl√ºck",
        "Musik", "Sport", "Spiel", "Film",
        "Foto", "Bild", "Farbe", "Gr√∂√üe", "Form",
        "Richtung", "Seite", "Teil", "St√ºck",
        "Land", "L√§nder", "Deutschland", "√ñsterreich", "Schweiz",
        "Information", "Programm", "Gruppe", "Klasse",
        "Familie", "Hochzeit", "Party", "Fest",
    ]
    tokens.extend(nouns_abstract)

    # ‚îÄ‚îÄ‚îÄ 11. –ü—Ä–∏–∫–º–µ—Ç–Ω–∏–∫–∏ (160) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    adjectives = [
        # –û—Å–Ω–æ–≤–Ω—ñ
        "gut", "schlecht", "sch√∂n", "h√§sslich",
        "gro√ü", "klein", "lang", "kurz",
        "alt", "neu", "jung",
        "hoch", "tief", "breit", "schmal",
        "schwer", "leicht",
        "schnell", "langsam",  # –≤–∂–µ —î —è–∫ –ø—Ä–∏—Å–ª—ñ–≤–Ω–∏–∫–∏, –∞–ª–µ –û–ö –¥–ª—è adj
        "warm", "kalt", "hei√ü", "k√ºhl",
        "hell", "dunkel",
        "laut", "leise",
        "nah", "weit", "n√§chst",
        "richtig", "falsch",
        "wichtig", "m√∂glich", "n√∂tig",
        "einfach", "schwierig", "kompliziert",
        "billig", "teuer", "g√ºnstig",
        "frei", "fertig", "bereit",
        "offen", "geschlossen",
        "voll", "leer",
        "sauber", "schmutzig",
        "gesund", "krank", "m√ºde",
        "hungrig", "durstig", "satt",
        "gl√ºcklich", "traurig", "zufrieden",
        "freundlich", "nett", "lustig",
        "interessant", "langweilig",
        "bekannt", "ber√ºhmt",
        "typisch", "normal", "besonder",
        "verschieden", "gleich", "√§hnlich",
        "letzt", "erst", "zweit", "dritt",
        "ganz", "halb",
        "andere", "anderer", "anderes",
        # –ö–æ–º–ø–∞—Ä–∞—Ç–∏–≤–∏ —Ç–∞ —Å—É–ø–µ—Ä–ª–∞—Ç–∏–≤–∏ (—á–∞—Å—Ç—ñ)
        "besser", "beste", "besten",
        "gr√∂√üer", "gr√∂√üte", "gr√∂√üten",
        "mehr", "meisten",
        "lieber", "liebsten",
        "h√∂her", "h√∂chste",
        "l√§nger", "l√§ngste",
        "√§lter", "√§lteste",
        "sch√∂ner", "sch√∂nste", "sch√∂nsten",
        "kleiner", "kleinste",
        # als, am ‚Äî –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω—å
        "als",
        # Partizip —è–∫ –ø—Ä–∏–∫–º–µ—Ç–Ω–∏–∫
        "interessiert", "verheiratet", "geschieden",
    ]
    tokens.extend(adjectives)

    # ‚îÄ‚îÄ‚îÄ 12. –ß–∏—Å–ª—ñ–≤–Ω–∏–∫–∏ (35) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    numerals = [
        "null", "eins", "zwei", "drei", "vier", "f√ºnf",
        "sechs", "sieben", "acht", "neun", "zehn",
        "elf", "zw√∂lf", "dreizehn", "vierzehn", "f√ºnfzehn",
        "zwanzig", "drei√üig", "vierzig", "f√ºnfzig",
        "sechzig", "siebzig", "achtzig", "neunzig",
        "hundert", "tausend", "Million",
        "erste", "zweite", "dritte", "vierte", "f√ºnfte",
        # –¶–∏—Ñ—Ä–∏ —è–∫ —Ç–æ–∫–µ–Ω–∏ (–¥–ª—è –≥–æ–¥–∏–Ω, –¥–∞—Ç)
        "0", "1", "2", "3", "4", "5", "6", "7", "8", "9",
        "10", "11", "12", "15", "20", "30",
    ]
    tokens.extend(numerals)

    # ‚îÄ‚îÄ‚îÄ 13. –°–ª—É–∂–±–æ–≤—ñ —Å–ª–æ–≤–∞ —Ç–∞ —á–∞—Å—Ç–∫–∏ (90) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    function_words = [
        # –ó–∞–ø–µ—Ä–µ—á–µ–Ω–Ω—è
        "nicht", "nichts", "niemand", "niemals",
        # –ù–µ–≤–∏–∑–Ω–∞—á–µ–Ω—ñ
        "etwas", "alles", "alle", "viele", "einige", "manche",
        "jemand", "niemand",
        "beide", "mehrere",
        # –ê—Ä—Ç–∏–∫–µ–ª—å–Ω—ñ + –∫—ñ–ª—å–∫—ñ—Å–Ω—ñ
        "mehr", "weniger", "genug",
        "jeder", "jede", "jedes",
        # –ú–æ–¥–∞–ª—å–Ω—ñ —á–∞—Å—Ç–∫–∏
        "doch", "mal", "ja", "nein", "denn",
        "eben", "halt", "wohl", "etwa", "eigentlich",
        # –ü—Ä–∏—Å–ª—ñ–≤–Ω–∏–∫–æ–≤—ñ
        "auch", "nur", "schon", "noch", "wieder",
        "zuerst", "zun√§chst", "endlich", "pl√∂tzlich",
        "nat√ºrlich", "leider", "hoffentlich",
        "vielleicht", "bestimmt", "sicher", "wahrscheinlich",
        "besonders", "wirklich", "ziemlich",
        "trotzdem", "deshalb", "deswegen", "darum",
        "au√üerdem", "√ºbrigens",
        # –ü—Ä–∏–π–º–µ–Ω–Ω–∏–∫–æ–≤—ñ —Å–ø–æ–ª—É–∫–∏
        "daf√ºr", "dagegen", "dazu", "davon", "damit",
        "dar√ºber", "darunter",
        # –î–æ–ø–æ–º—ñ–∂–Ω—ñ
        "es", "so", "zu", "am",
        "hin", "her",
        # –ù–∞–ø—Ä—è–º–∫–∏
        "nach", "Hause",  # nach Hause ‚Äî –¥—É–∂–µ —á–∞—Å—Ç–µ
    ]
    tokens.extend(function_words)

    # ‚îÄ‚îÄ‚îÄ 14. –°–ª–æ–≤–∞ –¥–ª—è –ø–æ—è—Å–Ω–µ–Ω—å —Ç—å—é—Ç–æ—Ä–∞ (50) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    tutor_words_ua = [
        "\n",
        # –£–∫—Ä–∞—ó–Ω—Å—å–∫—ñ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ—è—Å–Ω–µ–Ω—å
        "–ü–æ—è—Å–Ω–µ–Ω–Ω—è", "–ü–æ—è—Å–Ω–µ–Ω–Ω—è:",
        "–ü—Ä–∞–≤–∏–ª—å–Ω–æ", "–ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ",
        "–†–µ—á–µ–Ω–Ω—è", "–ø—Ä–∞–≤–∏–ª—å–Ω–µ", "–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–µ",
        "–ü–æ–º–∏–ª–∫–∞", "–ø–æ–º–∏–ª–∫–∞",
        "–¢—É—Ç", "—Ç—É—Ç", "—Ç—Ä–µ–±–∞", "–ø–æ—Ç—Ä—ñ–±–Ω–æ",
        "–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–≤–∞—Ç–∏", "–≤–∂–∏–≤–∞—Ç–∏", "–≤–∂–∏–≤–∞—î—Ç—å—Å—è", "–≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ",
        "–∑–∞–º—ñ—Å—Ç—å", "–±–æ", "–≤—ñ–¥", "–Ω–µ", "–≤", "–∞", "–∑",
        "–¥—ñ—î—Å–ª–æ–≤–æ", "–∞—Ä—Ç–∏–∫–ª—å", "—ñ–º–µ–Ω–Ω–∏–∫", "–ø—Ä–∏–π–º–µ–Ω–Ω–∏–∫", "–î—ñ—î—Å–ª–æ–≤–æ",
        "—Ñ–æ—Ä–º–∞", "—Ñ–æ—Ä–º—É", "—Ñ–æ—Ä–º—ñ", "–æ–∑–Ω–∞—á–∞—î", "—Ä—É—Ö", "—Ç–æ–º—É",
        "Dativ", "Akkusativ", "Nominativ",
        "Perfekt", "Pr√§sens", "Pr√§teritum",
        "Partizip", "II",
        "–¥–æ–ø–æ–º—ñ–∂–Ω–µ", "–æ—Å–Ω–æ–≤–Ω–µ", "–¥–æ–ø–æ–º—ñ–∂–Ω–æ–≥–æ",
        "–ø—ñ—Å–ª—è", "–ø–µ—Ä–µ–¥", "–Ω–∞–ø—Ä–∏–∫—ñ–Ω—Ü—ñ",
        "–º–∏–Ω—É–ª–æ–º—É", "—Ç–µ–ø–µ—Ä—ñ—à–Ω—å–æ–º—É", "—á–∞—Å—ñ",
        "–ø–æ—Ä—è–¥–æ–∫", "—Å–ª—ñ–≤", "–ø–æ–∑–∏—Ü—ñ—è",
        "–≤—ñ–¥–æ–∫—Ä–µ–º–ª—é–≤–∞–Ω–∞", "–ø—Ä–∏—Å—Ç–∞–≤–∫–∞",
        "mit", "sein", "haben",  # –¥—É–±–ª—ñ OK ‚Äî —ó—Ö –≤—ñ–¥—Ñ—ñ–ª—å—Ç—Ä—É—î–º–æ
        "–≤–∏–º–∞–≥–∞—î", "–∫–µ—Ä—É—î", "–ø–æ—Ç—Ä–µ–±—É—î",
        "–£", "—Ä–µ—á–µ–Ω–Ω—ñ", "–ö–æ–ª–∏", "–ø–æ—á–∏–Ω–∞—î—Ç—å—Å—è", "–º–∞—î", "—Å—Ç–æ—è—Ç–∏", "–Ω–∞", "–¥—Ä—É–≥–æ–º—É", "–º—ñ—Å—Ü—ñ", "–ø–µ—Ä–µ–¥", "–ø—ñ–¥–º–µ—Ç–æ–º", "–ø—ñ–¥–º–µ—Ç–∞",
        "—ñ–Ω—Ñ—ñ–Ω—ñ—Ç–∏–≤—ñ", "–±—É—Ç–∏", "–¥–ª—è", "–∑–∞–∫—ñ–Ω—á–µ–Ω–Ω—è", "–ø—Ä–∞–≤–∏–ª—å–Ω–æ",
        # –ó–∞–∫—ñ–Ω—á–µ–Ω–Ω—è –¥—ñ—î—Å–ª—ñ–≤
        "e", "st", "t", "en",
        "Correct:", "Incorrect:",
    ]
    tokens.extend(tutor_words_ua)

    # ‚îÄ‚îÄ‚îÄ 15. –î–æ–¥–∞—Ç–∫–æ–≤—ñ —á–∞—Å—Ç—ñ A2 —Å–ª–æ–≤–∞ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    extra_words = [
        # –°–ª–æ–≤–∞, —â–æ —á–∞—Å—Ç–æ –∑—É—Å—Ç—Ä—ñ—á–∞—é—Ç—å—Å—è –≤ A2 —Ç–µ–∫—Å—Ç–∞—Ö
        "bitte", "danke", "Entschuldigung", "tut",
        "Herr", "Frau", "Doktor",
        "Europa", "Welt",
        "Hobby", "Reise", "Ausflug", "Wanderung",
        "Ergebnis", "Erfolg",
        "Regel", "Regeln",
        "Recht", "Pflicht",
        "Kultur", "Tradition",
        "Umwelt",
        "Zukunft", "Vergangenheit",
        "Gesicht", "Stimme",
        "Geschenk", "√úberraschung",
        "Haushalt", "Miete", "Strom",
        "Bewerbung", "Lebenslauf",
        "Pass", "Ausweis", "Visum",
        "Vertrag", "Formular",
        "Ordnung", "Sicherheit",
        "Verkehr", "Unfall",
        "Erkl√§rung",
        "h√§tte", "w√§re", "k√∂nnte",
        "w√ºrde", "w√ºrden",
    ]
    tokens.extend(extra_words)

    # ‚îÄ‚îÄ‚îÄ 16. –î–æ–¥–∞—Ç–∫–æ–≤—ñ –¥—ñ—î—Å–ª—ñ–≤–Ω—ñ —Ñ–æ—Ä–º–∏ (wir/ihr/sie) (120) ‚îÄ
    # –ë–∞–≥–∞—Ç–æ –¥—ñ—î—Å–ª—ñ–≤ –≤–∏—â–µ –º–∞–ª–∏ –ª–∏—à–µ ich/du/er —Ñ–æ—Ä–º–∏.
    # –î–æ–¥–∞—î–º–æ wir/sie/ihr —Ñ–æ—Ä–º–∏ + Pr√§teritum –¥–ª—è —á–∞—Å—Ç–∏—Ö –¥—ñ—î—Å–ª—ñ–≤.
    extra_verb_forms = [
        # gehen Pr√§t.
        "ging", "gingen", "gingst",
        # kommen Pr√§t.
        "kam", "kamen", "kamst",
        # fahren Pr√§t.
        "fuhr", "fuhren",
        # sprechen Pr√§t.
        "sprach", "sprachen",
        # sehen Pr√§t.
        "sah", "sahen",
        # geben Pr√§t.
        "gab", "gaben",
        # nehmen Pr√§t.
        "nahm", "nahmen",
        # schreiben Pr√§t.
        "schrieb", "schrieben",
        # lesen Pr√§t.
        "las", "lasen",
        # finden Pr√§t.
        "fand", "fanden",
        # bringen Pr√§t.
        "brachte", "brachten",
        # denken Pr√§t.
        "dachte", "dachten",
        # essen Pr√§t.
        "a√ü", "a√üen",
        # trinken Pr√§t.
        "trank", "tranken",
        # schlafen Pr√§t.
        "schlief", "schliefen",
        # treffen Pr√§t.
        "traf", "trafen",
        # bleiben Pr√§t.
        "blieb", "blieben",
        # rufen Pr√§t.
        "rief", "riefen",
        # stehen Pr√§t.
        "stand", "standen",
        # sitzen Pr√§t.
        "sa√ü", "sa√üen",
        # liegen Pr√§t.
        "lag", "lagen",
        # laufen Pr√§t.
        "lief", "liefen",
        # fliegen Pr√§t.
        "flog", "flogen",
        # schwimmen Pr√§t.
        "schwamm", "schwammen",
        # tragen Pr√§t.
        "trug", "trugen",
        # helfen Pr√§t.
        "half", "halfen",
        # beginnen Pr√§t.
        "begann", "begannen",
        # vergessen Pr√§t.
        "verga√ü", "verga√üen",
        # fallen
        "fallen", "falle", "f√§llst", "f√§llt", "gefallen",
        "fiel", "fielen",
        # wachsen
        "wachsen", "w√§chst", "gewachsen",
        # rennen
        "rennen", "renne", "rennst", "rennt", "gerannt",
        # sterben
        "sterben", "stirbt", "gestorben",
        # passieren
        "passieren", "passiert",
        # erkl√§ren
        "erkl√§ren", "erkl√§re", "erkl√§rst", "erkl√§rt",
        # versuchen
        "versuchen", "versuche", "versuchst", "versucht",
        # entscheiden
        "entscheiden", "entscheide", "entscheidet", "entschieden",
        # erlauben
        "erlauben", "erlaube", "erlaubt",
        # empfehlen
        "empfehlen", "empfehle", "empfiehlt", "empfohlen",
        # bestellen
        "bestellen", "bestelle", "bestellst", "bestellt",
        # √ºbersetzen
        "√ºbersetzen", "√ºbersetze", "√ºbersetzt",
        # wiederholen
        "wiederholen", "wiederhole", "wiederholt",
        # reparieren
        "reparieren", "repariere", "repariert",
        # studieren
        "studieren", "studiere", "studierst", "studiert",
        # telefonieren
        "telefonieren", "telefoniere", "telefoniert",
        # funktionieren
        "funktionieren", "funktioniert",
        # reservieren
        "reservieren", "reserviert",
        # informieren
        "informieren", "informiert",
        # interessieren
        "interessieren", "interessiere", "interessiert",
        # probieren
        "probieren", "probiere", "probiert",
    ]
    tokens.extend(extra_verb_forms)

    # ‚îÄ‚îÄ‚îÄ 17. –ú–Ω–æ–∂–∏–Ω–∏ —ñ–º–µ–Ω–Ω–∏–∫—ñ–≤ —Ç–∞ –¥–æ–¥–∞—Ç–∫–æ–≤—ñ —ñ–º–µ–Ω–Ω–∏–∫–∏ (200) ‚îÄ
    extra_nouns = [
        # –ú–Ω–æ–∂–∏–Ω–∏ (Plural) ‚Äî —á–∞—Å—Ç—ñ
        "V√§ter", "M√ºtter", "S√∂hne", "T√∂chter", "Br√ºder", "Schwestern",
        "M√§nner", "Frauen", "Freunde", "Freundinnen",
        "H√§user", "Wohnungen", "Zimmer", "Tische", "St√ºhle",
        "B√ºcher", "Briefe", "S√§tze", "W√∂rter", "Texte",
        "Autos", "Busse", "Z√ºge", "Fahrr√§der",
        "St√§dte", "Stra√üen", "Pl√§tze", "Parks",
        "Bilder", "Filme", "Spiele", "Lieder",
        "Tiere", "Hunde", "Katzen", "V√∂gel",
        "Blumen", "B√§ume",
        "Probleme", "Fragen", "Antworten", "Ideen",
        "√Ñpfel", "Bananen", "Orangen",
        "Hemden", "Hosen", "Kleider", "Jacken", "M√§ntel",
        "Gesch√§fte", "Restaurants", "Hotels",
        "Kurse", "Pr√ºfungen", "Aufgaben", "√úbungen",
        "Termine", "Pl√§ne", "Ziele",
        "Preise", "Regeln",
        # –î–æ–¥–∞—Ç–∫–æ–≤—ñ —ñ–º–µ–Ω–Ω–∏–∫–∏ (A2 —Ç–µ–º–∏)
        "Ankunft", "Abfahrt", "Abflug", "Anschluss",
        "Eingang", "Ausgang", "Notausgang",
        "Erdgeschoss", "Stock", "Etage",
        "Schl√ºssel", "Rechnung", "Quittung",
        "Nachbar", "Vermieter", "Mieterin",
        "Rathaus", "Amt", "Beh√∂rde",
        "Kindergarten", "Spielplatz",
        "Einladung", "Geburtstag",
        "Abendessen", "Mittagessen",
        "Gabel", "Messer", "L√∂ffel", "Teller", "Glas", "Tasse",
        "Dose", "Flasche", "Packung", "St√ºck",
        "Seife", "Handtuch", "Zahnb√ºrste",
        "Bettw√§sche", "Decke", "Kissen",
        "Drucker", "Tastatur", "Bildschirm",
        "Waschmaschine", "K√ºhlschrank", "Herd", "Ofen",
        "Papier", "Schere", "Kleber",
        "Landkarte", "Stadtplan", "Fahrplan",
        "Fahrkarte", "Monatskarte",
        "Zeugnis", "Diplom", "Zertifikat",
        "Gehalt", "Lohn", "Rente",
        "Steuer", "Versicherung",
        "Woche", "Wochen",
        "Urlaub", "Reise",
        "Geburtstag", "Hochzeit",
        "Ruhe", "L√§rm", "Stress",
        "Praktikum", "Ausbildung",
    ]
    tokens.extend(extra_nouns)

    # ‚îÄ‚îÄ‚îÄ 18. Declined adjective endings + more adj forms (80) ‚îÄ
    extra_adjectives = [
        # –ß–∞—Å—Ç—ñ —Ñ–æ—Ä–º–∏ –∑ –≤—ñ–¥–º—ñ–Ω–∫–æ–≤–∏–º–∏ –∑–∞–∫—ñ–Ω—á–µ–Ω–Ω—è–º–∏
        "guter", "gutes", "guten", "gutem", "gute",
        "neuer", "neues", "neuen", "neuem", "neue",
        "alter", "altes", "alten", "altem", "alte",
        "gro√üer", "gro√ües", "gro√üen", "gro√üem", "gro√üe",
        "kleiner", "kleines", "kleinen", "kleinem", "kleine",
        "sch√∂ner", "sch√∂nes", "sch√∂nen", "sch√∂nem",
        "langer", "langes", "langen", "langem", "lange",
        # –©–µ –ø—Ä–∏–∫–º–µ—Ç–Ω–∏–∫–∏
        "praktisch", "gem√ºtlich", "bequem",
        "p√ºnktlich", "ordentlich", "h√∂flich",
        "gef√§hrlich", "ungef√§hrlich",
        "m√∂glich", "unm√∂glich",
        "notwendig", "dringend",
        "angenehm", "unangenehm",
        "zufrieden", "unzufrieden",
        "bekannt", "unbekannt",
        "verheiratet", "ledig",
        "arbeitslos", "berufst√§tig",
        "spannend", "aufregend",
        "ruhig", "nerv√∂s",
        "stolz", "b√∂se",
        "froh", "frisch",
        "trocken", "nass", "feucht",
        "dick", "d√ºnn",
        "eng", "locker",
        "weich", "hart",
        "s√º√ü", "sauer", "bitter", "scharf",
        "lecker",
        "kostenlos", "gratis",
    ]
    tokens.extend(extra_adjectives)

    # ‚îÄ‚îÄ‚îÄ 19. –ë—ñ–ª—å—à–µ —É–∫—Ä–∞—ó–Ω—Å—å–∫–∏—Ö —Å–ª—ñ–≤ –¥–ª—è —Ç—å—é—Ç–æ—Ä–∞ (100) ‚îÄ‚îÄ‚îÄ‚îÄ
    extra_tutor_ua = [
        # –ü–æ—è—Å–Ω–µ–Ω–Ω—è –≥—Ä–∞–º–∞—Ç–∏–∫–∏ —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é
        "—Ä–µ—á–µ–Ω–Ω—è", "—Å–ª–æ–≤–æ", "—Å–ª–æ–≤–∞",
        "–æ–∑–Ω–∞—á–µ–Ω–∏–π", "–Ω–µ–æ–∑–Ω–∞—á–µ–Ω–∏–π",
        "–æ–¥–Ω–∏–Ω–∞", "–º–Ω–æ–∂–∏–Ω–∞",
        "—á–æ–ª–æ–≤—ñ—á–∏–π", "–∂—ñ–Ω–æ—á–∏–π", "—Å–µ—Ä–µ–¥–Ω—ñ–π", "—Ä—ñ–¥",
        "–≤—ñ–¥–º—ñ–Ω–æ–∫", "–Ω–∞–∑–∏–≤–Ω–∏–π", "–∑–Ω–∞—Ö—ñ–¥–Ω–∏–π", "–¥–∞–≤–∞–ª—å–Ω–∏–π",
        "–ø—ñ–¥–º–µ—Ç", "–ø—Ä–∏—Å—É–¥–æ–∫", "–¥–æ–¥–∞—Ç–æ–∫",
        "–≥–æ–ª–æ–≤–Ω–µ", "–ø—ñ–¥—Ä—è–¥–Ω–µ",
        "—Å–ø–æ–ª—É—á–Ω–∏–∫", "—á–∞—Å—Ç–∫–∞",
        "–ø—Ä–∞–≤–∏–ª—å–Ω–∏–π", "–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∏–π",
        "–º–∏–Ω—É–ª–∏–π", "—Ç–µ–ø–µ—Ä—ñ—à–Ω—ñ–π", "—á–∞—Å",
        "–¥–æ–ø–æ–º—ñ–∂–Ω–∏–π",
        "–¥—ñ—î—Å–ª–æ–≤–∞", "—Ä—É—Ö—É", "—Å—Ç–∞–Ω—É",
        "–∑–∞–∫—ñ–Ω—á–µ–Ω–Ω—è", "–æ—Å–Ω–æ–≤–∞", "–∫–æ—Ä—ñ–Ω—å",
        "–ø—Ä–µ—Ñ—ñ–∫—Å", "—Å—É—Ñ—ñ–∫—Å",
        "–Ω–∞–≥–æ–ª–æ—Å", "–≤–∏–º–æ–≤–∞",
        "–∑–Ω–∞—á–µ–Ω–Ω—è", "–ø–µ—Ä–µ–∫–ª–∞–¥",
        "–ø—Ä–∏–∫–ª–∞–¥", "–ø—Ä–∞–≤–∏–ª–æ",
        "–≤–∏–Ω—è—Ç–æ–∫", "–≤–∏–Ω—è—Ç–∫–∏",
        "–∑–∞–ø–µ—Ä–µ—á–µ–Ω–Ω—è", "—Å—Ç–≤–µ—Ä–¥–∂–µ–Ω–Ω—è",
        "–∑–∞–ø–∏—Ç–∞–Ω–Ω—è", "–≤—ñ–¥–ø–æ–≤—ñ–¥—å",
        "–ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è", "—Å—Ç—É–ø—ñ–Ω—å",
        "–≤–∏—â–∏–π", "–Ω–∞–π–≤–∏—â–∏–π",
        "–ø—Ä—è–º–∏–π", "–Ω–µ–ø—Ä—è–º–∏–π",
        "–≤–≤—ñ—á–ª–∏–≤–∞", "–Ω–µ—Ñ–æ—Ä–º–∞–ª—å–Ω–∞",
        "—Ñ–æ—Ä–º–∞–ª—å–Ω–∞", "–∑–≤–µ—Ä—Ç–∞–Ω–Ω—è",
        "–ø–æ—Å–ª—ñ–¥–æ–≤–Ω—ñ—Å—Ç—å", "—Å—Ç—Ä—É–∫—Ç—É—Ä–∞",
        "—Å—Ç–æ—ó—Ç—å", "—Å—Ç–æ—è—Ç–∏",
        "–Ω–∞–ø—Ä–∏–∫—ñ–Ω—Ü—ñ", "–Ω–∞", "–ø–æ—á–∞—Ç–∫—É",
        "–¥—Ä—É–≥—É", "–¥—Ä—É–≥—ñ–π", "—Ç—Ä–µ—Ç—é",
        "–¥—Ä—É–≥–æ–º—É", "–º—ñ—Å—Ü—ñ",
        "—ñ–Ω—Ñ—ñ–Ω—ñ—Ç–∏–≤", "–¥—ñ—î–≤—ñ–¥–º—ñ–Ω–∞",
        "–≤—ñ–¥–º—ñ–Ω—é—î—Ç—å—Å—è", "–∑–º—ñ–Ω—é—î—Ç—å—Å—è",
        "–º–æ–¥–∞–ª—å–Ω–µ", "—Å–º–∏—Å–ª–æ–≤–µ",
        "–∑–Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è", "–ø–µ—Ä–µ–º—ñ—â—É—î—Ç—å—Å—è",
        "–≤–∫–∞–∑—É—î", "–æ–∑–Ω–∞—á–∞—î",
        "–ø–æ—Ç—Ä–µ–±—É—î", "–≤–∏–º–∞–≥–∞—î",
        "–ø—Ä–∞–≤–∏–ª—å–Ω–∞", "–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∞",
        "–í—ñ–¥–ø–æ–≤—ñ–¥—å", "–í–∏–ø—Ä–∞–≤–ª–µ–Ω–Ω—è",
        # –®–∞–±–ª–æ–Ω–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π
        "–≤–∂–∏–≤–∞—Ç–∏", "–∑",
        "—É", "—Ü—å–æ–º—É", "–≤–∏–ø–∞–¥–∫—É",
        "—Ç–æ–º—É", "—â–æ",
        "–º–∞—î", "–±—É—Ç–∏",
        "—Å—Ç–æ—è—Ç–∏", "–∫—ñ–Ω—Ü—ñ",
    ]
    tokens.extend(extra_tutor_ua)

    # ‚îÄ‚îÄ‚îÄ 20. –°–∫–ª–∞–¥–Ω—ñ / –∫–æ–º–ø–æ–∑–∏—Ç–Ω—ñ —Å–ª–æ–≤–∞ —Ç–∞ —Ä–µ—à—Ç–∞ (150) ‚îÄ‚îÄ‚îÄ‚îÄ
    compound_and_misc = [
        # –°–∫–ª–∞–¥–Ω—ñ —Å–ª–æ–≤–∞, —á–∞—Å—Ç—ñ –≤ A2
        "Hauptbahnhof", "Einkaufszentrum",
        "Arbeitgeber", "Arbeitnehmer",
        "Muttersprache", "Fremdsprache",
        "Sprachkurs", "Deutschkurs",
        "Arbeitsplatz", "Parkplatz",
        "Kinderzimmer", "Esszimmer", "Arbeitszimmer",
        "Einfamilienhaus", "Mehrfamilienhaus",
        "Briefkasten", "M√ºlleimer",
        "Geburtsort", "Geburtsdatum",
        "Familienstand", "Staatsangeh√∂rigkeit",
        "Aufenthaltserlaubnis",
        # –ö–æ–ª—ñ—Ä
        "rot", "blau", "gr√ºn", "gelb", "schwarz", "wei√ü",
        "braun", "grau", "rosa", "orange", "lila",
        # –ù–∞–ø—Ä—è–º–∫–∏ / —Å—Ç–æ—Ä–æ–Ω–∏
        "Norden", "S√ºden", "Westen", "Osten",
        "n√∂rdlich", "s√ºdlich", "westlich", "√∂stlich",
        # –ú–∞—Ç–µ—Ä—ñ–∞–ª–∏ —Ç–∞ —Ä–µ—á–æ–≤–∏–Ω–∏
        "Holz", "Metall", "Glas", "Plastik", "Stoff",
        # –ü—Ä–∏–π–æ–º–∏ —ó–∂—ñ / –∫—É—Ö–Ω—è
        "backen", "braten", "grillen", "schneiden",
        "gebacken", "gebraten", "gegrillt", "geschnitten",
        # –©–µ –¥—ñ—î—Å–ª–æ–≤–∞ (—á–∞—Å—Ç—ñ)
        "aufpassen", "aufgepasst",
        "nachdenken", "nachgedacht",
        "umziehen", "umgezogen",
        "spazieren", "spaziert",
        "wandern", "gewandert",
        "tanzen", "getanzt",
        "singen", "gesungen",
        "l√§cheln", "gel√§chelt",
        "weinen", "geweint",
        "lachen", "gelacht",
        "tr√§umen", "getr√§umt",
        "f√ºhlen", "gef√ºhlt",
        "merken", "gemerkt",
        "bemerken", "bemerkt",
        "vermissen", "vermisst",
        "st√∂ren", "gest√∂rt",
        "nutzen", "genutzt",
        "sammeln", "gesammelt",
        "teilen", "geteilt",
        "warten", "gewartet",
        "suchen", "gesucht",
        "verlieren", "verloren",
        "gewinnen", "gewonnen",
        "ziehen", "gezogen",
        "dr√ºcken", "gedr√ºckt",
        "schieben", "geschoben",
        "h√§ngen", "geh√§ngt",
        "schenken", "geschenkt",
        "schicken", "geschickt",
        "liefern", "geliefert",
        "buchen", "gebucht",
        "anmelden", "angemeldet",
        "abmelden", "abgemeldet",
        "unterschreiben", "unterschrieben",
        "ausf√ºllen", "ausgef√ºllt",
        # –©–µ –ø—Ä–∏—Å–ª—ñ–≤–Ω–∏–∫–∏ / —á–∞—Å—Ç–∫–∏
        "normalerweise", "meistens", "wenigstens",
        "mindestens", "h√∂chstens",
        "irgendwo", "irgendwann", "irgendwie",
        "nirgendwo", "nirgends",
        "sofort", "gleich",
        "inzwischen", "zwischendurch",
        "hinterher", "vorher",
        "obendrein", "insgesamt",
        "allerdings", "jedoch",
        "n√§mlich", "beispielsweise",
        "ungef√§hr",
        # –©–µ –ø—Ä–∏–π–º–µ–Ω–Ω–∏–∫–æ–≤—ñ —Ñ—Ä–∞–∑–∏
        "ums", "f√ºrs", "aufs", "√ºbers",
        # –ì—Ä–∞–º. —Ç–µ—Ä–º—ñ–Ω–∏ (–¥–ª—è —Ç—å—é—Ç–æ—Ä–∞, –Ω—ñ–º. –º–æ–≤–æ—é)
        "Verb", "Nomen", "Adjektiv", "Adverb",
        "Artikel", "Pronomen", "Pr√§position",
        "Konjunktion", "Subjekt", "Objekt",
        "Singular", "Plural",
        "maskulin", "feminin", "neutral",
        "Endung", "Stamm", "Vorsilbe",
        "Nebensatz", "Hauptsatz",
        "Infinitiv", "Imperativ",
        "Genitiv",
    ]
    tokens.extend(compound_and_misc)

    # ‚îÄ‚îÄ‚îÄ 21. –ì–µ–æ–≥—Ä–∞—Ñ—ñ—è —Ç–∞ –∫—Ä–∞—ó–Ω–∏ (–¥–ª—è –ø—Ä–∏–∫–ª–∞–¥—ñ–≤) (50) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    geography = [
        # –ú—ñ—Å—Ç–∞ –ù—ñ–º–µ—á—á–∏–Ω–∏
        "Berlin", "M√ºnchen", "Hamburg", "K√∂ln", "Frankfurt",
        "Stuttgart", "D√ºsseldorf", "Dresden", "Leipzig", "Hannover",
        "Bremen", "N√ºrnberg", "Bonn", "Heidelberg", "Freiburg",
        # –Ü–Ω—à—ñ –∫—Ä–∞—ó–Ω–∏/–º—ñ—Å—Ç–∞
        "Wien", "Z√ºrich", "Bern", "Salzburg",
        "Paris", "London", "Rom", "Madrid", "Moskau",
        "T√ºrkei", "Spanien", "Italien", "Frankreich", "Polen",
        "Russland", "Ukraine", "Griechenland", "Kroatien",
        "Asien", "Afrika", "Amerika",
        # –ì–µ–æ–≥—Ä–∞—Ñ—ñ—á–Ω—ñ –ø–æ–Ω—è—Ç—Ç—è
        "Rhein", "Donau", "Alpen", "Nordsee", "Ostsee",
        "Insel", "Inseln", "Gebirge", "K√ºste",
        "Stadtrand", "Stadtzentrum", "Altstadt",
    ]
    tokens.extend(geography)

    # ‚îÄ‚îÄ‚îÄ 22. –ß–∞—Å—Ç—ñ —ñ–º–µ–Ω–∞ –¥–ª—è –≤–ø—Ä–∞–≤ (40) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    names = [
        "Anna", "Maria", "Peter", "Thomas", "Michael",
        "Hans", "Klaus", "Karl", "Stefan", "Martin",
        "Julia", "Sabine", "Monika", "Petra", "Andrea",
        "Ali", "Fatima", "Mohammed", "Olga", "Sergei",
        "Max", "Felix", "Laura", "Sophie", "Emma",
        "Leon", "Lena", "Tim", "Lisa", "David",
        "M√ºller", "Schmidt", "Fischer", "Weber", "Meyer",
        "Herr", "Frau",  # already exist but dedup handles it
    ]
    tokens.extend(names)

    # ‚îÄ‚îÄ‚îÄ 23. –†–µ—Ñ–ª–µ–∫—Å–∏–≤–Ω—ñ –¥—ñ—î—Å–ª–æ–≤–∞ (30) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    reflexive_verbs = [
        "waschen",  # sich waschen
        "anziehen", "angezogen", "ausziehen", "ausgezogen",
        "hinsetzen", "hingesetzt",
        "hinlegen", "hingelegt",
        "beeilen", "beeilt",
        "beschweren", "beschwert",
        "erinnern", "erinnert",
        "entschuldigen", "entschuldigt",
        "unterhalten", "unterhalten",
        "verabreden", "verabredet",
        "versp√§ten", "versp√§tet",
        "konzentrieren", "konzentriert",
        "gew√∂hnen", "gew√∂hnt",
        "befinden", "befindet",
        "besch√§ftigen", "besch√§ftigt",
    ]
    tokens.extend(reflexive_verbs)

    # ‚îÄ‚îÄ‚îÄ 24. –©–µ –¥—ñ—î—Å–ª–æ–≤–∞, —è–∫–∏—Ö –Ω–µ –≤–∏—Å—Ç–∞—á–∞—î (60) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    more_verbs = [
        "wechseln", "gewechselt",
        "verdienen", "verdient",
        "ausgeben", "ausgegeben",
        "sparen", "gespart",
        "leihen", "geliehen",
        "berichten", "berichtet",
        "beschreiben", "beschrieben",
        "diskutieren", "diskutiert",
        "planen", "geplant",
        "organisieren", "organisiert",
        "kontrollieren", "kontrolliert",
        "korrigieren", "korrigiert",
        "√ºberpr√ºfen", "√ºberpr√ºft",
        "vergleichen", "verglichen",
        "verbessern", "verbessert",
        "verschenken", "verschenkt",
        "versprechen", "versprochen",
        "vorhaben", "vorgehabt",
        "zuh√∂ren", "zugeh√∂rt",
        "aufwachen", "aufgewacht",
        "einschlafen", "eingeschlafen",
        "duschen", "geduscht",
        "fr√ºhst√ºcken", "gefr√ºhst√ºckt",
        "unterrichten", "unterrichtet",
        "√ºbersetzen", # already in vocab
        "ausdr√ºcken", "ausgedr√ºckt",
        "heiraten", "geheiratet",
        "trennen", "getrennt",
        "streiten", "gestritten",
    ]
    tokens.extend(more_verbs)

    # ‚îÄ‚îÄ‚îÄ 25. –©–µ —ñ–º–µ–Ω–Ω–∏–∫–∏ —Ç–∞ –º–Ω–æ–∂–∏–Ω–∏ (100) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    more_nouns = [
        # –û—Å–≤—ñ—Ç–∞
        "Abitur", "Studium", "Semester", "Vorlesung",
        "Professor", "Professorin", "Dozent", "Dozentin",
        "Note", "Noten", "Fehler",
        # –ü–æ–±—É—Ç
        "M√ºll", "M√ºlleimer", "Staubsauger", "B√ºgeleisen",
        "Geschirr", "Geschirrsp√ºler", "Sp√ºlmaschine",
        "Dusche", "Toilette", "Waschbecken",
        # –§—ñ–Ω–∞–Ω—Å–∏
        "Konto", "Sparkonto", "√úberweisung", "Kredit",
        "Schulden", "Bargeld", "Kreditkarte", "Bankkarte",
        # –ú–µ–¥—ñ–∞
        "Radio", "Sendung", "Nachrichten", "Werbung",
        "Kanal", "Serie", "Folge",
        # –ü–æ–¥–æ—Ä–æ–∂—ñ
        "Gep√§ck", "Handgep√§ck", "Koffer",  # Koffer already exists
        "Reservierung", "Unterkunft", "Pension",
        "Jugendherberge", "Campingplatz",
        "Sehensw√ºrdigkeit", "Rundfahrt", "Stadtf√ºhrung",
        # –ê–¥–º—ñ–Ω—ñ—Å—Ç—Ä–∞—Ç–∏–≤–Ω—ñ
        "Anmeldung", "Abmeldung", "Bescheinigung",
        "F√ºhrerschein", "Personalausweis",
        "Meldebescheinigung", "Antrag",
        # –©–µ
        "Projekt", "Ergebnis",  # Ergebnis already exists
        "Beitrag", "Bericht", "Vortrag",
        "Eingang", "Ausgang",  # may exist
        "Empfang", "Rezeption",
        "Trinkgeld", "Bedienung",
        "Speisekarte", "Vorspeise", "Hauptgericht", "Nachspeise",
        "Getr√§nkekarte", "Rechnung",
        "Vegetarier", "Allergiker",
        "Portion", "Scheibe", "Dose",  # Dose may exist
        "Kanne", "Becher",
        "Spieler", "Mannschaft", "Verein",
        "Stadion", "Halle", "Schwimmbad",
        "Training", "Wettkampf", "Meisterschaft",
        "Nachricht", "Anruf", "Anrufbeantworter",
        "Absender", "Empf√§nger", "Betreff",
        "Anhang", "Datei", "Dokument",
        "Drucker",  # may exist
        "Bildung", "Wissen", "Kenntnis",
    ]
    tokens.extend(more_nouns)

    # ‚îÄ‚îÄ‚îÄ 26. –©–µ –ø—Ä–∏–∫–º–µ—Ç–Ω–∏–∫–∏ —Ç–∞ –ø—Ä–∏—Å–ª—ñ–≤–Ω–∏–∫–∏ (50) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    more_adj_adv = [
        "direkt", "indirekt",
        "automatisch", "elektrisch",
        "international", "national",
        "privat", "√∂ffentlich",
        "modern", "klassisch",
        "original", "aktuell",
        "regelm√§√üig", "unregelm√§√üig",
        "pers√∂nlich", "beruflich",
        "schriftlich", "m√ºndlich",
        "t√§glich", "w√∂chentlich", "monatlich", "j√§hrlich",
        "doppelt", "einzeln",
        "gemeinsam", "getrennt",
        "selbstst√§ndig", "abh√§ngig",
        "anwesend", "abwesend",
        "einverstanden",
        "neugierig", "√§ngstlich", "w√ºtend",
        "ernst", "komisch",
        "verr√ºckt", "verantwortlich",
        "tats√§chlich", "offensichtlich",
        "haupts√§chlich", "grunds√§tzlich",
        "ausgezeichnet", "hervorragend",
        "anst√§ndig", "vern√ºnftig",
        "dankbar", "ehrlich",
        "geduldig", "ungeduldig",
        "aufmerksam",
        "irgendein", "irgendeine",
    ]
    tokens.extend(more_adj_adv)

    # ‚îÄ‚îÄ‚îÄ 27. –°–ª–æ–≤–∞ –∑ –ø—ñ–¥—Ä—É—á–Ω–∏–∫–∞ Begegnungen A2 (PDF) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –¥–æ–¥–∞—î–º–æ —Ä–µ–∞–ª—å–Ω—ñ A2 —Å–ª–æ–≤–∞, —è–∫–∏—Ö —â–µ –Ω–µ–º–∞—î –≤ —Å–ª–æ–≤–Ω–∏–∫—É
    pdf_words = _extract_pdf_words()
    tokens.extend(pdf_words)

    # ‚îÄ‚îÄ‚îÄ –î–µ–¥—É–ø–ª—ñ–∫–∞—Ü—ñ—è —Ç–∞ –ø–æ–±—É–¥–æ–≤–∞ —Å–ª–æ–≤–Ω–∏–∫–∞ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    seen: set[str] = set()
    unique_tokens: list[str] = []
    for t in tokens:
        if t not in seen:
            seen.add(t)
            unique_tokens.append(t)

    # Target size: V from config
    config = load_config()
    TARGET_V = config.model.vocab_size
    
    while len(unique_tokens) < TARGET_V:
        reserved = f"<RESERVED_{len(unique_tokens)}>"
        unique_tokens.append(reserved)

    # If more ‚Äî trim (keep first TARGET_V)
    if len(unique_tokens) > TARGET_V:
        unique_tokens = unique_tokens[:TARGET_V]

    vocab = {token: idx for idx, token in enumerate(unique_tokens)}
    return vocab


def _extract_pdf_words() -> list[str]:
    """–í–∏—Ç—è–≥—É—î —Ä–µ–∞–ª—å–Ω—ñ A2 —Å–ª–æ–≤–∞ –∑ –ø—ñ–¥—Ä—É—á–Ω–∏–∫–∞ Begegnungen A2 (PDF).

    –§—ñ–ª—å—Ç—Ä—É—î:
    - URL-–∏ —Ç–∞ –∫–æ–¥–∏ (wordwall, learningapps, https, www‚Ä¶)
    - –ó–∞–Ω–∞–¥—Ç–æ –∫–æ—Ä–æ—Ç–∫—ñ —Å–ª–æ–≤–∞ (< 3 —Å–∏–º–≤–æ–ª–∏)
    - –ì—Ä–∞–º–∞—Ç–∏—á–Ω—ñ —Å–∫–æ—Ä–æ—á–µ–Ω–Ω—è (Akk, Dat, Pl‚Ä¶)

    Returns:
        –°–ø–∏—Å–æ–∫ —Å–ª—ñ–≤ –∑ PDF, –≤—ñ–¥—Å–æ—Ä—Ç–æ–≤–∞–Ω–∏–π –∑–∞ —á–∞—Å—Ç–æ—Ç–Ω—ñ—Å—Ç—é.
    """
    import re
    from collections import Counter

    pdf_path = Path(__file__).parent.parent.parent / "data_raw" / "Begegnungen_–ê2.pdf"
    if not pdf_path.exists():
        print("  ‚ö†Ô∏è  PDF not found ‚Äî skipping PDF word extraction")
        return []

    try:
        import fitz  # PyMuPDF
    except ImportError:
        print("  ‚ö†Ô∏è  PyMuPDF not installed ‚Äî skipping PDF extraction")
        return []

    doc = fitz.open(str(pdf_path))
    all_text = ""
    for page in doc:
        all_text += page.get_text() + " "
    doc.close()

    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –≤—Å—ñ —Å–ª–æ–≤–∞ (2+ —Å–∏–º–≤–æ–ª–∏, –∑ —É–º–ª–∞—É—Ç–∞–º–∏)
    words = re.findall(r"[A-Za-z√Ñ√§√ñ√∂√ú√º√ü]{3,}", all_text)
    freq = Counter(words)

    # –°—Ç–æ–ø-—Å–ª–æ–≤–∞: URL-—Ñ—Ä–∞–≥–º–µ–Ω—Ç–∏, –∫–æ–¥–∏, —Å–∫–æ—Ä–æ—á–µ–Ω–Ω—è
    junk = {
        "https", "http", "www", "net", "org", "com",
        "wordwall", "resource", "learningapps",
        "QR", "USB", "Akk", "Dat", "THEMA", "WORTSCHATZ",
        "GRAMMATIK", "VERTIEFUNGSTEIL", "Investor", "Selma",
        "Nico", "Pauli", "Codes", "overgeordneten",
        "√ºbergeordneten",
    }

    result = []
    for word, count in freq.most_common():
        if count < 2:
            break
        if word in junk:
            continue
        result.append(word)

    return result


def print_stats(vocab: dict[str, int]) -> None:
    """–í–∏–≤–æ–¥–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Å–ª–æ–≤–Ω–∏–∫—É."""
    print(f"{'='*50}")
    print(f"  Vocab size: {len(vocab)} tokens")
    print(f"{'='*50}")
    # –ü–æ–∫–∞–∂–µ–º–æ –ø–µ—Ä—à—ñ —Ç–∞ –æ—Å—Ç–∞–Ω–Ω—ñ —Ç–æ–∫–µ–Ω–∏
    items = list(vocab.items())
    print(f"\n  First 10: {items[:10]}")
    print(f"  Last  10: {items[-10:]}")
    print()


if __name__ == "__main__":
    vocab = build_vocab()

    if "--stats" in sys.argv:
        print_stats(vocab)
    else:
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —É —Ñ–∞–π–ª (–ø–æ—Ä—É—á –∑—ñ —Å–∫—Ä–∏–ø—Ç–æ–º)
        save_path = Path(__file__).parent / "vocab.json"
        with open(save_path, "w", encoding="utf-8") as f:
            json.dump(vocab, f, ensure_ascii=False, indent=2)
        print(f"‚úÖ vocab.json saved ‚Äî {len(vocab)} tokens")
