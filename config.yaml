# ── A2 Deutsch Grammar Tutor ── Model & Training Config ──

model:
  vocab_size: 4000 # V — curated word-level tokens (A2 forms + PDF words)
  max_seq_len: 64 # T — context window
  d_model: 128 # embedding / hidden dimension
  n_layers: 4 # L — decoder blocks
  n_heads: 4 # H — attention heads
  d_ff: 512 # FFN hidden dimension
  weight_tying: true # tie embeddings and LM head

training:
  batch_size: 64
  learning_rate: 3e-4
  epochs: 20
  early_stopping_patience: 5  # stop if val loss has not improved for this many epochs (0 = disabled)
  decision_token_weight: 5.0  # extra weight for ✅/❌/Correct/Incorrect tokens in loss (1.0 = no boost)
  warmup_steps: 500
  device: "auto" # "auto" = best of cuda/xpu/mps/cpu, or set "cuda"|"xpu"|"mps"|"cpu"

data:
  train_path: "data/train.jsonl"
  val_path: "data/val.jsonl"

generation:
  temperature: 0.3
  top_k: 50
