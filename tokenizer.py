"""
tokenizer.py â€” Word-level Ñ‚Ğ¾ĞºĞµĞ½Ñ–Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ´Ğ»Ñ A2 German Grammar Tutor.

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Ğ©Ğ Ğ¢ĞĞšĞ• Ğ¢ĞĞšĞ•ĞĞ†Ğ—ĞĞ¢ĞĞ ?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ¿Ñ€Ğ°Ñ†ÑÑ” Ğ»Ğ¸ÑˆĞµ Ğ· Ñ‡Ğ¸ÑĞ»Ğ°Ğ¼Ğ¸. Ğ¢Ğ¾ĞºĞµĞ½Ñ–Ğ·Ğ°Ñ‚Ğ¾Ñ€ â€” Ñ†Ğµ Â«Ğ¿ĞµÑ€ĞµĞºĞ»Ğ°Ğ´Ğ°Ñ‡Â»
Ğ¼Ñ–Ğ¶ Ğ»ÑĞ´ÑÑŒĞºĞ¸Ğ¼ Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼ Ñ– Ñ‡Ğ¸ÑĞ»Ğ°Ğ¼Ğ¸ (ID):

    encode("Ich bin mÃ¼de.") â†’ [4, 60, 469, 4]      Ñ‚ĞµĞºÑÑ‚ â†’ Ñ‡Ğ¸ÑĞ»Ğ°
    decode([4, 60, 469, 4]) â†’ "Ich bin mÃ¼de."       Ñ‡Ğ¸ÑĞ»Ğ° â†’ Ñ‚ĞµĞºÑÑ‚

Ğ¯Ğº Ñ†Ğµ Ğ¿Ñ€Ğ°Ñ†ÑÑ”:
    1. Ğ¢ĞµĞºÑÑ‚ Ñ€Ğ¾Ğ·Ğ±Ğ¸Ğ²Ğ°Ñ”Ñ‚ÑŒÑÑ Ğ½Ğ° ÑĞ»Ğ¾Ğ²Ğ° (Ñ‚Ğ¾ĞºĞµĞ½Ğ¸)
    2. ĞšĞ¾Ğ¶Ğ½Ğµ ÑĞ»Ğ¾Ğ²Ğ¾ ÑˆÑƒĞºĞ°Ñ”Ñ‚ÑŒÑÑ Ñƒ ÑĞ»Ğ¾Ğ²Ğ½Ğ¸ĞºÑƒ (vocab.json)
    3. Ğ¯ĞºÑ‰Ğ¾ ÑĞ»Ğ¾Ğ²Ğ¾ Ğ·Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğµ â†’ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ‚Ğ°Ñ”Ğ¼Ğ¾ Ğ¹Ğ¾Ğ³Ğ¾ ID
    4. Ğ¯ĞºÑ‰Ğ¾ Ğ½Ğµ Ğ·Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğµ â†’ Ğ¿Ğ¾Ğ²ĞµÑ€Ñ‚Ğ°Ñ”Ğ¼Ğ¾ ID <UNK> (unknown)

Ğ¡Ğ¿ĞµÑ†Ñ–Ğ°Ğ»ÑŒĞ½Ñ– Ñ‚Ğ¾ĞºĞµĞ½Ğ¸:
    <PAD> (id=0) â€” Ğ·Ğ°Ğ¿Ğ¾Ğ²Ğ½ĞµĞ½Ğ½Ñ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ñ… Ğ¿Ğ¾ÑĞ»Ñ–Ğ´Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚ĞµĞ¹ Ğ´Ğ¾ Ğ¾Ğ´Ğ½Ñ–Ñ”Ñ— Ğ´Ğ¾Ğ²Ğ¶Ğ¸Ğ½Ğ¸
    <BOS> (id=1) â€” "beginning of sequence" â€” Ğ¼Ğ°Ñ€ĞºĞµÑ€ Ğ¿Ğ¾Ñ‡Ğ°Ñ‚ĞºÑƒ Ñ‚ĞµĞºÑÑ‚Ñƒ
    <EOS> (id=2) â€” "end of sequence" â€” Ğ¼Ğ°Ñ€ĞºĞµÑ€ ĞºÑ–Ğ½Ñ†Ñ Ñ‚ĞµĞºÑÑ‚Ñƒ
    <UNK> (id=3) â€” "unknown" â€” Ğ·Ğ°Ğ¼Ñ–Ğ½ÑÑ” Ğ±ÑƒĞ´ÑŒ-ÑĞºĞµ Ğ½ĞµĞ²Ñ–Ğ´Ğ¾Ğ¼Ğµ ÑĞ»Ğ¾Ğ²Ğ¾

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Ğ§ĞĞœĞ£ WORD-LEVEL, Ğ ĞĞ• BPE/SENTENCEPIECE?
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Ğ”Ğ»Ñ Ğ²ĞµĞ»Ğ¸ĞºĞ¸Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (GPT, LLaMA) Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒÑÑ‚ÑŒ sub-word Ñ‚Ğ¾ĞºĞµĞ½Ñ–Ğ·Ğ°Ñ‚Ğ¾Ñ€Ğ¸
(BPE), ÑĞºÑ– Ñ€Ğ¾Ğ·Ğ±Ğ¸Ğ²Ğ°ÑÑ‚ÑŒ ÑĞ»Ğ¾Ğ²Ğ° Ğ½Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸Ğ½Ğ¸: "gegangen" â†’ "ge" + "gang" + "en".

ĞœĞ¸ Ğ¾Ğ±Ñ€Ğ°Ğ»Ğ¸ word-level, Ñ‚Ğ¾Ğ¼Ñƒ Ñ‰Ğ¾:
    âœ… ĞŸÑ€Ğ¾ÑÑ‚Ñ–ÑˆĞµ Ğ´Ğ»Ñ Ğ½Ğ°Ğ²Ñ‡Ğ°Ğ½Ğ½Ñ Ñ‚Ğ° Ñ€Ğ¾Ğ·ÑƒĞ¼Ñ–Ğ½Ğ½Ñ
    âœ… V=2000 Ğ´Ğ¾ÑÑ‚Ğ°Ñ‚Ğ½ÑŒĞ¾ Ğ´Ğ»Ñ A2 (Ğ¾Ğ±Ğ¼ĞµĞ¶ĞµĞ½Ğ° Ğ»ĞµĞºÑĞ¸ĞºĞ°)
    âœ… ĞšĞ¾Ğ¶ĞµĞ½ Ñ‚Ğ¾ĞºĞµĞ½ = Ñ†Ñ–Ğ»Ğµ ÑĞ»Ğ¾Ğ²Ğ¾ â†’ Ğ»ĞµĞ³ÑˆĞµ Ñ–Ğ½Ñ‚ĞµÑ€Ğ¿Ñ€ĞµÑ‚ÑƒĞ²Ğ°Ñ‚Ğ¸
    âŒ ĞœÑ–Ğ½ÑƒÑ: Ğ½ĞµĞ²Ñ–Ğ´Ğ¾Ğ¼Ñ– ÑĞ»Ğ¾Ğ²Ğ° â†’ <UNK> (Ğ½Ğµ Ğ¼Ğ¾Ğ¶Ğµ Â«Ğ²Ğ³Ğ°Ğ´Ğ°Ñ‚Ğ¸Â» Ğ·Ğ° Ñ‡Ğ°ÑÑ‚Ğ¸Ğ½Ğ°Ğ¼Ğ¸)

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import json
import re
from pathlib import Path


# Ğ¢ĞµĞ½Ğ·Ğ¾Ñ€Ğ½Ñ– Ñ„Ğ¾Ñ€Ğ¼Ğ¸ (tensor shapes) Ğ´Ğ»Ñ Ñ†ÑŒĞ¾Ğ³Ğ¾ ĞµÑ‚Ğ°Ğ¿Ñƒ:
#
#   Input text:     "Ich bin mÃ¼de."
#   After encode:   [1, 60, 155, 469, 4, 2]          shape: [seq_len]
#                    â†‘                    â†‘
#                   BOS                  EOS
#
#   Batch (padding): [[1, 60, 155, 469, 4, 2],        shape: [batch_size, max_seq_len]
#                     [1, 22, 88,  4,   2, 0]]         â† 0 = PAD
#                                              â†‘
#                                             PAD


class Tokenizer:
    """Word-level Ñ‚Ğ¾ĞºĞµĞ½Ñ–Ğ·Ğ°Ñ‚Ğ¾Ñ€ Ğ· Ñ„Ñ–ĞºÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğ¼ ÑĞ»Ğ¾Ğ²Ğ½Ğ¸ĞºĞ¾Ğ¼.

    ĞœĞ°Ñ‚Ñ€Ğ¸Ñ‡Ğ½Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ:
        vocab â€” Ñ†Ğµ Ğ²Ñ–Ğ´Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ½Ñ (mapping): str â†’ int
        Embedding-ÑˆĞ°Ñ€ Ğ¿Ğ¾Ñ‚Ñ–Ğ¼ Ğ¿ĞµÑ€ĞµÑ‚Ğ²Ğ¾Ñ€Ğ¸Ñ‚ÑŒ int â†’ vector [d_model]

        Ğ›Ğ°Ğ½Ñ†ÑĞ³:  Ñ‚ĞµĞºÑÑ‚ â†’ Tokenizer â†’ [idâ‚, idâ‚‚, â€¦] â†’ Embedding â†’ [[vâ‚], [vâ‚‚], â€¦]
                  str       â†“          list[int]          â†“         [seq_len, d_model]
    """

    # Ğ¡Ğ¿ĞµÑ†Ñ–Ğ°Ğ»ÑŒĞ½Ñ– Ñ‚Ğ¾ĞºĞµĞ½Ğ¸ â€” ĞºĞ¾Ğ½ÑÑ‚Ğ°Ğ½Ñ‚Ğ¸
    PAD_TOKEN = "<PAD>"
    BOS_TOKEN = "<BOS>"
    EOS_TOKEN = "<EOS>"
    UNK_TOKEN = "<UNK>"

    def __init__(self, vocab_path: str | Path = "vocab.json"):
        """Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ÑƒÑ” ÑĞ»Ğ¾Ğ²Ğ½Ğ¸Ğº Ğ· JSON-Ñ„Ğ°Ğ¹Ğ»Ñƒ.

        Args:
            vocab_path: ÑˆĞ»ÑÑ… Ğ´Ğ¾ vocab.json (token â†’ id)

        Ğ’Ğ½ÑƒÑ‚Ñ€Ñ–ÑˆĞ½Ñ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ°:
            self.token_to_id: {"ich": 60, "bin": 155, ...}  â€” Ğ´Ğ»Ñ encode
            self.id_to_token: {60: "ich", 155: "bin", ...}  â€” Ğ´Ğ»Ñ decode
        """
        vocab_path = Path(vocab_path)
        if not vocab_path.exists():
            raise FileNotFoundError(
                f"Vocab file not found: {vocab_path}\n"
                f"Run 'python build_vocab.py' to create it."
            )

        with open(vocab_path, "r", encoding="utf-8") as f:
            self.token_to_id: dict[str, int] = json.load(f)

        # Ğ—Ğ²Ğ¾Ñ€Ğ¾Ñ‚Ğ½Ñ–Ğ¹ Ğ¼Ğ°Ğ¿Ğ¿Ñ–Ğ½Ğ³: id â†’ token (Ğ´Ğ»Ñ decode)
        self.id_to_token: dict[int, str] = {
            idx: token for token, idx in self.token_to_id.items()
        }

        # Ğ—Ğ±ĞµÑ€Ñ–Ğ³Ğ°Ñ”Ğ¼Ğ¾ ID ÑĞ¿ĞµÑ†Ñ–Ğ°Ğ»ÑŒĞ½Ğ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ñ–Ğ² Ğ´Ğ»Ñ ÑˆĞ²Ğ¸Ğ´ĞºĞ¾Ğ³Ğ¾ Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ñƒ
        self.pad_id = self.token_to_id[self.PAD_TOKEN]   # 0
        self.bos_id = self.token_to_id[self.BOS_TOKEN]   # 1
        self.eos_id = self.token_to_id[self.EOS_TOKEN]   # 2
        self.unk_id = self.token_to_id[self.UNK_TOKEN]   # 3

    @property
    def vocab_size(self) -> int:
        """Ğ Ğ¾Ğ·Ğ¼Ñ–Ñ€ ÑĞ»Ğ¾Ğ²Ğ½Ğ¸ĞºĞ° â€” ĞºÑ–Ğ»ÑŒĞºÑ–ÑÑ‚ÑŒ ÑƒĞ½Ñ–ĞºĞ°Ğ»ÑŒĞ½Ğ¸Ñ… Ñ‚Ğ¾ĞºĞµĞ½Ñ–Ğ².

        Ğ¦Ğµ Ñ‡Ğ¸ÑĞ»Ğ¾ Ğ²Ğ¸Ğ·Ğ½Ğ°Ñ‡Ğ°Ñ” Ñ€Ğ¾Ğ·Ğ¼Ñ–Ñ€ embedding-Ğ¼Ğ°Ñ‚Ñ€Ğ¸Ñ†Ñ–:
            Embedding matrix shape = [vocab_size, d_model] = [~2000, 128]
        """
        return len(self.token_to_id)

    def _tokenize(self, text: str) -> list[str]:
        """Ğ Ğ¾Ğ·Ğ±Ğ¸Ğ²Ğ°Ñ” Ñ‚ĞµĞºÑÑ‚ Ğ½Ğ° ÑĞ¿Ğ¸ÑĞ¾Ğº Ñ‚Ğ¾ĞºĞµĞ½Ñ–Ğ² (ÑĞ»Ñ–Ğ² + Ğ¿ÑƒĞ½ĞºÑ‚ÑƒĞ°Ñ†Ñ–Ñ).

        Ğ’Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒÑ” regex Ğ´Ğ»Ñ Ñ€Ğ¾Ğ·Ğ´Ñ–Ğ»ĞµĞ½Ğ½Ñ:
        - Ğ¡Ğ»Ğ¾Ğ²Ğ° (Ğ· ÑƒĞ¼Ğ»Ğ°ÑƒÑ‚Ğ°Ğ¼Ğ¸: Ã¤, Ã¶, Ã¼, ÃŸ)
        - ĞŸÑƒĞ½ĞºÑ‚ÑƒĞ°Ñ†Ñ–Ñ Ğ¾ĞºÑ€ĞµĞ¼Ğ¾ (. , ! ? : ;)
        - Emoji-Ğ¼Ğ°Ñ€ĞºĞµÑ€Ğ¸ (âœ…, âŒ, ğŸ“)
        - Ğ¡Ğ¿ĞµÑ†Ñ–Ğ°Ğ»ÑŒĞ½Ñ– ÑĞ»Ğ¾Ğ²Ğ° Ğ· Ğ´Ğ²Ğ¾ĞºÑ€Ğ°Ğ¿ĞºĞ¾Ñ (Correct:, Explanation:)
        - Ğ¡Ğ¸Ğ¼Ğ²Ğ¾Ğ» Ğ½Ğ¾Ğ²Ğ¾Ğ³Ğ¾ Ñ€ÑĞ´ĞºĞ° \\n

        ĞŸÑ€Ğ¸ĞºĞ»Ğ°Ğ´:
            "Ich bin mÃ¼de." â†’ ["Ich", "bin", "mÃ¼de", "."]
            "âŒ Incorrect." â†’ ["âŒ", "Incorrect", "."]
        """
        # ĞŸĞ¾Ñ€ÑĞ´Ğ¾Ğº Ğ°Ğ»ÑŒÑ‚ĞµÑ€Ğ½Ğ°Ñ‚Ğ¸Ğ² Ğ²Ğ°Ğ¶Ğ»Ğ¸Ğ²Ğ¸Ğ¹: ÑĞ¿ĞµÑ€ÑˆÑƒ Ğ´Ğ¾Ğ²ÑˆÑ– Ğ¿Ğ°Ñ‚ĞµÑ€Ğ½Ğ¸!
        pattern = (
            r"Correct:|Incorrect\.|Explanation:|ĞŸĞ¾ÑÑĞ½ĞµĞ½Ğ½Ñ:"  # multi-char specials
            r"|\.\.\."                                        # Ñ‚Ñ€Ğ¸ ĞºÑ€Ğ°Ğ¿ĞºĞ¸ (...)
            r"|[âœ…âŒğŸ“]"                                      # emoji-Ğ¼Ğ°Ñ€ĞºĞµÑ€Ğ¸
            r"|\n"                                            # Ğ½Ğ¾Ğ²Ğ¸Ğ¹ Ñ€ÑĞ´Ğ¾Ğº
            r"|[A-Za-zÃ„Ã¤Ã–Ã¶ÃœÃ¼ÃŸ\u0400-\u04FF]+"               # ÑĞ»Ğ¾Ğ²Ğ°
            r"|[.,!?;:\"'\-()]"                               # Ğ¿ÑƒĞ½ĞºÑ‚ÑƒĞ°Ñ†Ñ–Ñ
        )
        tokens = re.findall(pattern, text)
        return [t for t in tokens if t]  # filter empty strings just in case

    def encode(
        self,
        text: str,
        add_bos: bool = True,
        add_eos: bool = True,
        max_len: int | None = None,
    ) -> list[int]:
        """ĞŸĞµÑ€ĞµÑ‚Ğ²Ğ¾Ñ€ÑÑ” Ñ‚ĞµĞºÑÑ‚ Ñƒ Ğ¿Ğ¾ÑĞ»Ñ–Ğ´Ğ¾Ğ²Ğ½Ñ–ÑÑ‚ÑŒ ID.

        Tensor shape: [seq_len]  â€” Ğ¾Ğ´Ğ½Ğ¾Ğ²Ğ¸Ğ¼Ñ–Ñ€Ğ½Ğ¸Ğ¹ Ğ²ĞµĞºÑ‚Ğ¾Ñ€

        Args:
            text: Ğ²Ñ…Ñ–Ğ´Ğ½Ğ¸Ğ¹ Ñ‚ĞµĞºÑÑ‚
            add_bos: Ñ‡Ğ¸ Ğ´Ğ¾Ğ´Ğ°Ğ²Ğ°Ñ‚Ğ¸ <BOS> Ğ½Ğ° Ğ¿Ğ¾Ñ‡Ğ°Ñ‚Ğ¾Ğº (Ğ·Ğ°Ğ·Ğ²Ğ¸Ñ‡Ğ°Ğ¹ True)
            add_eos: Ñ‡Ğ¸ Ğ´Ğ¾Ğ´Ğ°Ğ²Ğ°Ñ‚Ğ¸ <EOS> Ğ² ĞºÑ–Ğ½ĞµÑ†ÑŒ (Ğ·Ğ°Ğ·Ğ²Ğ¸Ñ‡Ğ°Ğ¹ True)
            max_len: Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ° Ğ´Ğ¾Ğ²Ğ¶Ğ¸Ğ½Ğ° (Ğ¾Ğ±Ñ€Ñ–Ğ·Ğ°Ñ”, ÑĞºÑ‰Ğ¾ Ğ´Ğ¾Ğ²ÑˆĞµ)

        Returns:
            list[int] â€” Ğ¿Ğ¾ÑĞ»Ñ–Ğ´Ğ¾Ğ²Ğ½Ñ–ÑÑ‚ÑŒ token ID

        ĞŸÑ€Ğ¸ĞºĞ»Ğ°Ğ´:
            encode("Ich bin mÃ¼de.")
            â†’ tokenize: ["Ich", "bin", "mÃ¼de", "."]
            â†’ lookup:   [60, 155, 469, 4]
            â†’ + BOS/EOS: [1, 60, 155, 469, 4, 2]
        """
        raw_tokens = self._tokenize(text)

        ids: list[int] = []
        if add_bos:
            ids.append(self.bos_id)

        for token in raw_tokens:
            # Ğ¨ÑƒĞºĞ°Ñ”Ğ¼Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½ Ñƒ ÑĞ»Ğ¾Ğ²Ğ½Ğ¸ĞºÑƒ
            token_id = self.token_to_id.get(token)
            if token_id is not None:
                ids.append(token_id)
            else:
                # Ğ¡Ğ¿Ñ€Ğ¾Ğ±ÑƒÑ”Ğ¼Ğ¾ lowercase (ÑĞºÑ‰Ğ¾ Â«HeuteÂ» Ğ½Ğµ Ğ·Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾, ÑˆÑƒĞºĞ°Ñ”Ğ¼Ğ¾ Â«heuteÂ»)
                token_id = self.token_to_id.get(token.lower())
                if token_id is not None:
                    ids.append(token_id)
                else:
                    ids.append(self.unk_id)  # Ğ½ĞµĞ²Ñ–Ğ´Ğ¾Ğ¼Ğµ ÑĞ»Ğ¾Ğ²Ğ¾ â†’ <UNK>

        if add_eos:
            ids.append(self.eos_id)

        # ĞĞ±Ñ€Ñ–Ğ·Ğ°Ñ”Ğ¼Ğ¾ Ğ´Ğ¾ max_len (Ğ²ĞºĞ»ÑÑ‡Ğ½Ğ¾ Ğ· BOS/EOS)
        if max_len is not None and len(ids) > max_len:
            ids = ids[:max_len]
            # ĞŸĞµÑ€ĞµĞºĞ¾Ğ½ÑƒÑ”Ğ¼Ğ¾ÑÑŒ Ñ‰Ğ¾ Ğ¾ÑÑ‚Ğ°Ğ½Ğ½Ñ–Ğ¹ Ñ‚Ğ¾ĞºĞµĞ½ â€” EOS
            if add_eos:
                ids[-1] = self.eos_id

        return ids

    def decode(self, ids: list[int], skip_special: bool = True) -> str:
        """ĞŸĞµÑ€ĞµÑ‚Ğ²Ğ¾Ñ€ÑÑ” Ğ¿Ğ¾ÑĞ»Ñ–Ğ´Ğ¾Ğ²Ğ½Ñ–ÑÑ‚ÑŒ ID Ğ½Ğ°Ğ·Ğ°Ğ´ Ñƒ Ñ‚ĞµĞºÑÑ‚.

        Args:
            ids: ÑĞ¿Ğ¸ÑĞ¾Ğº token ID
            skip_special: ÑĞºÑ‰Ğ¾ True, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°Ñ” <PAD>, <BOS>, <EOS>, <UNK>

        Returns:
            str â€” Ğ²Ñ–Ğ´Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğ¹ Ñ‚ĞµĞºÑÑ‚

        ĞŸÑ€Ğ¸ĞºĞ»Ğ°Ğ´:
            decode([1, 60, 155, 469, 4, 2]) â†’ "Ich bin mÃ¼de."
        """
        special_ids = {self.pad_id, self.bos_id, self.eos_id}
        tokens: list[str] = []

        for token_id in ids:
            if skip_special and token_id in special_ids:
                continue
            token = self.id_to_token.get(token_id, self.UNK_TOKEN)
            tokens.append(token)

        # Ğ—Ê¼Ñ”Ğ´Ğ½ÑƒÑ”Ğ¼Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ¸ Ğ½Ğ°Ğ·Ğ°Ğ´ Ñƒ Ñ‚ĞµĞºÑÑ‚
        # ĞŸÑƒĞ½ĞºÑ‚ÑƒĞ°Ñ†Ñ–Ñ Ğ¿Ñ€Ğ¸Ñ”Ğ´Ğ½ÑƒÑ”Ñ‚ÑŒÑÑ Ğ±ĞµĞ· Ğ¿Ñ€Ğ¾Ğ±Ñ–Ğ»Ñƒ Ğ¿ĞµÑ€ĞµĞ´ Ğ½ĞµÑ
        if not tokens:
            return ""

        result = tokens[0]
        for token in tokens[1:]:
            if token in {".", ",", "!", "?", ":", ";", ")", '"', "'", "..."}:
                result += token
            elif token == "\n":
                result += token
            elif result.endswith("(") or result.endswith('"') or result.endswith("\n"):
                result += token
            else:
                result += " " + token

        return result

    def pad_sequence(
        self, ids: list[int], max_len: int, pad_id: int | None = None
    ) -> list[int]:
        """Ğ”Ğ¾Ğ¿Ğ¾Ğ²Ğ½ÑÑ” Ğ¿Ğ¾ÑĞ»Ñ–Ğ´Ğ¾Ğ²Ğ½Ñ–ÑÑ‚ÑŒ PAD-Ñ‚Ğ¾ĞºĞµĞ½Ğ°Ğ¼Ğ¸ Ğ´Ğ¾ Ğ¿Ğ¾Ñ‚Ñ€Ñ–Ğ±Ğ½Ğ¾Ñ— Ğ´Ğ¾Ğ²Ğ¶Ğ¸Ğ½Ğ¸.

        ĞĞ°Ğ²Ñ–Ñ‰Ğ¾ padding?
        ĞĞµĞ¹Ñ€Ğ¾Ğ¼ĞµÑ€ĞµĞ¶Ğ° Ğ¾Ğ±Ñ€Ğ¾Ğ±Ğ»ÑÑ” Ğ´Ğ°Ğ½Ñ– Ğ±Ğ°Ñ‚Ñ‡Ğ°Ğ¼Ğ¸ (Ğ¿Ğ°Ñ‡ĞºĞ°Ğ¼Ğ¸).
        Ğ’ÑÑ– Ğ¿Ğ¾ÑĞ»Ñ–Ğ´Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ñ– Ğ² Ğ±Ğ°Ñ‚Ñ‡Ñ– Ğ¼Ğ°ÑÑ‚ÑŒ Ğ±ÑƒÑ‚Ğ¸ Ğ¾Ğ´Ğ½Ñ–Ñ”Ñ— Ğ´Ğ¾Ğ²Ğ¶Ğ¸Ğ½Ğ¸:

            Batch (Ğ´Ğ¾ padding):
                [1, 60, 155, 469, 4, 2]        â† 6 Ñ‚Ğ¾ĞºĞµĞ½Ñ–Ğ²
                [1, 22, 88,  4,   2]            â† 5 Ñ‚Ğ¾ĞºĞµĞ½Ñ–Ğ²  â† Ğ Ğ†Ğ—ĞĞ Ğ”ĞĞ’Ğ–Ğ˜ĞĞ!

            Batch (Ğ¿Ñ–ÑĞ»Ñ padding Ğ´Ğ¾ max_len=6):
                [1, 60, 155, 469, 4, 2]         â† 6 Ñ‚Ğ¾ĞºĞµĞ½Ñ–Ğ²
                [1, 22, 88,  4,   2, 0]         â† 6 Ñ‚Ğ¾ĞºĞµĞ½Ñ–Ğ²  â† 0 = PAD

            Tensor shape: [batch_size=2, max_seq_len=6]
        """
        if pad_id is None:
            pad_id = self.pad_id

        if len(ids) >= max_len:
            return ids[:max_len]
        return ids + [pad_id] * (max_len - len(ids))

    def __repr__(self) -> str:
        return f"Tokenizer(vocab_size={self.vocab_size}, path=vocab.json)"


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SELF-TEST: Ğ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸ python tokenizer.py Ğ´Ğ»Ñ Ğ¿ĞµÑ€ĞµĞ²Ñ–Ñ€ĞºĞ¸
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    print("=" * 60)
    print("  ğŸ§ª Tokenizer Self-Test")
    print("=" * 60)

    tok = Tokenizer("vocab.json")
    print(f"\n  âœ… Loaded: {tok}")
    print(f"  Special IDs: PAD={tok.pad_id}, BOS={tok.bos_id}, EOS={tok.eos_id}, UNK={tok.unk_id}")

    # â”€â”€â”€ Test 1: Basic encode/decode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\nâ”€â”€â”€ Test 1: Encode / Decode â”€â”€â”€")
    test_sentence = "Ich bin mÃ¼de."
    encoded = tok.encode(test_sentence)
    decoded = tok.decode(encoded)
    print(f"  Input:   '{test_sentence}'")
    print(f"  Encoded: {encoded}  (shape: [{len(encoded)}])")
    print(f"  Decoded: '{decoded}'")

    # â”€â”€â”€ Test 2: Unknown words â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\nâ”€â”€â”€ Test 2: Unknown Words â”€â”€â”€")
    test_unk = "Ich spiele Klavier."
    encoded_unk = tok.encode(test_unk)
    decoded_unk = tok.decode(encoded_unk, skip_special=False)
    print(f"  Input:   '{test_unk}'")
    print(f"  Encoded: {encoded_unk}")
    print(f"  Decoded: '{decoded_unk}'")
    unk_count = encoded_unk.count(tok.unk_id)
    print(f"  UNK tokens: {unk_count}")

    # â”€â”€â”€ Test 3: Tutor response format â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\nâ”€â”€â”€ Test 3: Tutor Response â”€â”€â”€")
    tutor_output = "âŒ Incorrect.\nâœ… Correct: Ich bin nach Hause gegangen.\nğŸ“ Explanation: gehen Ğ²Ğ¶Ğ¸Ğ²Ğ°Ñ”Ñ‚ÑŒÑÑ Ğ· sein."
    encoded_tutor = tok.encode(tutor_output)
    decoded_tutor = tok.decode(encoded_tutor)
    print(f"  Input:   '{tutor_output}'")
    print(f"  Encoded: {encoded_tutor}  (len={len(encoded_tutor)})")
    print(f"  Decoded: '{decoded_tutor}'")

    # â”€â”€â”€ Test 4: Padding â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\nâ”€â”€â”€ Test 4: Padding â”€â”€â”€")
    short = tok.encode("Ich lerne.")
    padded = tok.pad_sequence(short, max_len=10)
    print(f"  Original:  {short}  (len={len(short)})")
    print(f"  Padded:    {padded}  (len={len(padded)})")

    # â”€â”€â”€ Test 5: Roundtrip â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\nâ”€â”€â”€ Test 5: Roundtrip â”€â”€â”€")
    sentences = [
        "Ich habe gegessen.",
        "Heute gehe ich in die Schule.",
        "Er kann gut Deutsch sprechen.",
        "Wir sind nach Berlin gefahren.",
    ]
    all_ok = True
    for s in sentences:
        enc = tok.encode(s)
        dec = tok.decode(enc)
        ok = "âœ…" if dec == s else "âŒ"
        if dec != s:
            all_ok = False
        print(f"  {ok} '{s}' â†’ {enc} â†’ '{dec}'")

    # â”€â”€â”€ Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    print("\n" + "=" * 60)
    if all_ok:
        print("  âœ… All tests passed!")
    else:
        print("  âš ï¸  Some roundtrip tests had differences (may be OK for punctuation)")
    print(f"  Vocab size: {tok.vocab_size}")
    print("=" * 60)
