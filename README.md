# ğŸ‡©ğŸ‡ª A2 Deutsch Grammar Tutor

A compact Transformer model for checking and explaining errors in German sentences at the A2 level. 
Trained to correct grammar and provide simple explanations **in Ukrainian**.

ğŸ¤— **Model on Hugging Face:** [kengurukleo/deutsch_a2_transformer](https://huggingface.co/kengurukleo/deutsch_a2_transformer)  
âœ¨ **Live Demo (Space):** [kengurukleo/deutsch-a2-tutor](https://huggingface.co/spaces/kengurukleo/deutsch-a2-tutor)

## Features

| Function | Example |
|---|---|
| âœ… Validity Check | `Ich bin nach Hause gegangen.` â†’ âœ… |
| âŒ Error Correction | `Dann ich gehe...` â†’ `Dann gehe ich...` |
| ğŸ“ Explanations | Detailed grammar feedback provided in **Ukrainian**. |

### âœ… Covered A1-A2 Topics (100% Complete)

| Topic | Level | What the model does / Explanation | Status |
|:---|:---:|:---|:---:|
| **PrÃ¤sens** | A1 | Verb conjugation (e.g., *ich esse, du isst*). | âœ… |
| **W-Fragen** | A1 | Question word order (*Wo wohnst du?*). | âœ… |
| **Akkusativ** | A1 | Masculine article change (*der -> den*). | âœ… |
| **Negation** | A1 | Usage of *nicht* vs. *kein* (noun vs. adj/verb). | âœ… |
| **Modalverben** | A1/A2 | Conjugation and putting the main verb at the end. | âœ… |
| **Possessivpron.**| A1/A2 | Agreement of *mein, dein, sein...* in Nominativ. | âœ… |
| **Fixed Prepos.** | A1/A2 | Prepositions with fixed cases (*mit + Dat*, *fÃ¼r + Akk*). | âœ… |
| **Imperativ** | A1/A2 | Command forms for *du*, *ihr*, and *Sie*. | âœ… |
| **Perfekt** | A2 | Choosing *haben* vs. *sein* and *Partizip II* forms. | âœ… |
| **Inversion** | A2 | Verb-second rule after adverbs (*Heute gehe ich...*). | âœ… |
| **Separable Verbs**| A2 | Prefix position in present tense (*Ich kaufe ein*). | âœ… |
| **Dativ** | A2 | Articles after Dativ-governing verbs (*helfen, danken*). | âœ… |
| **WechselprÃ¤p.** | A2 | Two-way prepositions (Wohin? -> Akk / Wo? -> Dat). | âœ… |
| **NebensÃ¤tze** | A2 | Verb-last order in clauses with *weil, dass, wenn*. | âœ… |
| **Adjektivdekl.** | A2 | Basic endings after *ein* in Nominativ. | âœ… |
| **Reflexive Verben**| A2 | Correct reflexive pronouns (*mich, dich, sich...*). | âœ… |
| **PrÃ¤teritum** | A2 | Past tense forms for *sein* and *haben* (*war, hatte*). | âœ… |
| **Komparation** | A2 | Adjective comparison (*gut - besser*, not *mehr gut*). | âœ… |
| **Nominativ** | A1 | Article as subject (*Der/Die/Das* + noun). | âœ… |
| **Genitiv** | A2 | Genitive with prepositions (*wÃ¤hrend des*, *wegen der*, *trotz des*). | âœ… |

For a complete list of examples and model explanations for each topic, see:  
ğŸ‘‰ **[Grammar Topics & Examples](docs/topics_examples.md)**

## Architecture

```
Transformer Decoder Only
â”œâ”€â”€ V = 4,000 tokens (words + forms + explanations)
â”œâ”€â”€ T = 64  (max sequence length)
â”œâ”€â”€ d_model = 128
â”œâ”€â”€ L = 4 Layers
â”œâ”€â”€ H = 4 Attention Heads
â”œâ”€â”€ Weight tying = ON (shared weights between Embeddings and LM Head)
â””â”€â”€ Precision = FP16 â†’ Model size â‰ˆ 2.5 MB

Detailed mathematical description of all matrix transformations can be found in [docs/architecture.md](docs/architecture.md).
```

## Project Structure

```text
A2-Deutsch-Transformer/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ model.py                # Core Transformer architecture
â”‚   â”‚   â”œâ”€â”€ configuration_custom.py # HF Config wrapper
â”‚   â”‚   â””â”€â”€ modeling_custom.py      # HF Model wrapper (custom code)
â”‚   â”œâ”€â”€ tokenizer/
â”‚   â”‚   â”œâ”€â”€ build_vocab.py          # PDF analysis and vocab creation
â”‚   â”‚   â””â”€â”€ tokenizer.py            # Word-level tokenizer
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ generator.py            # Main synthetic data generator
â”‚   â”‚   â””â”€â”€ generators/             # Specialized topic generators
â”‚   â”œâ”€â”€ train.py                    # Training loop (MPS optimized)
â”‚   â”œâ”€â”€ generate.py                 # CLI inference script
â”‚   â””â”€â”€ export_hf.py                # Hugging Face export script
â”œâ”€â”€ hf_export/                      # Bundle for HF Hub (weights + code)
â”œâ”€â”€ hf_space/                       # Bundle for HF Spaces (Gradio app)
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_model.py              # Architecture and device tests
â”‚   â”œâ”€â”€ evaluate_model.py          # Full evaluation on 248 test examples
â”‚   â””â”€â”€ test_data.json              # Hand-crafted test sentences per topic
â”œâ”€â”€ data/                           # Generated JSONL datasets
â”œâ”€â”€ data_raw/                       # Raw PDF textbooks
â”œâ”€â”€ docs/                           # Architecture and grammar docs
â”œâ”€â”€ config.yaml                     # Model & training hyperparameters
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## How It Works

### 1. `build_vocab.py`
Creates the "brain" of the tokenizer. It analyzes the `Begegnungen_A2.pdf` textbook, extracts the most frequent German words, adds conjugation tables, and includes words for explanations. The result is a `vocab.json` file with 4000 unique tokens.

### 2. `generator.py`
Generates thousands of training examples. It knows grammar rules, takes a correct sentence and intentionally "breaks" it (e.g., changes word order or auxiliary verb), adding an explanation of why it is an error.

### 3. Training
The model is trained locally on **Apple Silicon (M1/M2/M3)** using `torch.device("mps")`. Due to its small size (2.5 MB), training takes only a few minutes.

## Installation & Setup

Follow these steps to initialize the project and set up the environment:

```bash
# 1. Clone the repository (if not already done)
git clone https://github.com/KenguruKleo/A2-Deutsch-Transformer.git
cd A2-Deutsch-Transformer

# 2. Create a virtual environment
python3 -m venv .venv

# 3. Activate the virtual environment
source .venv/bin/activate  # On macOS/Linux
# .venv\Scripts\activate     # On Windows

# 4. Install dependencies
pip install --upgrade pip
pip install -r requirements.txt
```

## Testing

To verify the model architecture and device compatibility, run the following command:

```bash
# Run model unit tests
python tests/test_model.py
```

These tests check:
- Output dimensions (`[batch, seq_len, vocab_size]`).
- Successful execution on **MPS** (Apple Silicon) or **CPU**.

## Quick Start

Once the environment is set up and activated:

```bash
# 1. Build vocabulary (if changing word lists)
python src/tokenizer/build_vocab.py

# 2. Generate training data
python src/data/generator.py

# 3. Run training
python src/train.py

# 4. Test the model
python src/generate.py --text "Ich habe nach Berlin gefahren."

# 5. Export to Hugging Face format
python src/export_hf.py
```

## Evaluation

The project includes an evaluation script that runs the trained model on **248 hand-crafted test examples** and measures detection accuracy (correct vs incorrect) and correction accuracy (whether the suggested fix matches the expected one).  
**Note:** The model is never trained on this test set; the examples are held out to measure generalization.

### How to run

```bash
# Activate the environment first
source .venv/bin/activate   # macOS/Linux

# Evaluate the default model (model_final.pth)
python tests/evaluate_model.py

# Verbose: print each test case and model response
python tests/evaluate_model.py --verbose

# Use a specific checkpoint
python tests/evaluate_model.py --model path/to/checkpoint.pth
```

### Results (example run)

| Metric | Value |
|--------|--------|
| Test examples | 248 |
| **Detection accuracy** | **248/248 (100.0%)** |
| **Correction accuracy** | **242/248 (97.6%)** |
| Correct â†’ detected as correct | 138/138 (100.0%) |
| Incorrect â†’ detected as incorrect | 110/110 (100.0%) |

**Per level:**  
- A1: Detection 99/99 (100.0%) | Correction 98/99 (99.0%)  
- A2: Detection 149/149 (100.0%) | Correction 144/149 (96.6%)

**Per-topic breakdown:**

| Topic | Total | Det.âœ… | Det.% | Corr.âœ… | Corr.% |
|-------|-------|--------|-------|--------|--------|
| adjective_endings | 8 | 8 | 100.0% | 8 | 100.0% |
| akkusativ | 19 | 19 | 100.0% | 19 | 100.0% |
| dativ | 12 | 12 | 100.0% | 12 | 100.0% |
| fixed_prepositions | 17 | 17 | 100.0% | 17 | 100.0% |
| genitiv | 4 | 4 | 100.0% | 2 | 50.0% |
| haben_sein | 17 | 17 | 100.0% | 16 | 94.1% |
| imperativ | 5 | 5 | 100.0% | 5 | 100.0% |
| inversion | 16 | 16 | 100.0% | 15 | 93.8% |
| komparation | 6 | 6 | 100.0% | 6 | 100.0% |
| modal | 15 | 15 | 100.0% | 15 | 100.0% |
| nebensatz_dass_wenn | 11 | 11 | 100.0% | 11 | 100.0% |
| nebensatz_weil | 6 | 6 | 100.0% | 6 | 100.0% |
| negation | 12 | 12 | 100.0% | 12 | 100.0% |
| nominativ | 6 | 6 | 100.0% | 6 | 100.0% |
| partizip | 7 | 7 | 100.0% | 6 | 85.7% |
| perfekt_aux | 20 | 20 | 100.0% | 20 | 100.0% |
| possessive | 8 | 8 | 100.0% | 8 | 100.0% |
| praesens | 13 | 13 | 100.0% | 13 | 100.0% |
| praeteritum | 8 | 8 | 100.0% | 8 | 100.0% |
| questions | 8 | 8 | 100.0% | 8 | 100.0% |
| reflexive | 5 | 5 | 100.0% | 5 | 100.0% |
| separable | 8 | 8 | 100.0% | 7 | 87.5% |
| strong_verbs | 6 | 6 | 100.0% | 6 | 100.0% |
| wechselpraep | 11 | 11 | 100.0% | 11 | 100.0% |

Detailed results are written to `tests/eval_results.json`. At the end of each run, the script prints **Failed Detection** cases (if any): sentences where the modelâ€™s correct/incorrect decision did not match the expected one.

## Hugging Face Hub

This model is hosted on the [Hugging Face Hub](https://huggingface.co/kengurukleo/deutsch_a2_transformer). Below are instructions for users and developers.

### ğŸ“¥ For Users (Loading the Model)
You can load and use this model directly in your Python code using the `transformers` library. Note that `trust_remote_code=True` is required because the model uses a custom architecture and tokenizer.

```python
from transformers import AutoModelForCausalLM

# Load model and use custom code from the Hub
model = AutoModelForCausalLM.from_pretrained(
    "kengurukleo/deutsch_a2_transformer", 
    trust_remote_code=True
)
```

### ğŸ›  For Developers (Export and Publish)
If you want to re-export the model or publish your own version:

1. **Export to Safetensors**:
   Run the export script to create a compatible bundle (weights, config, and source code):
   ```bash
   python src/export_hf.py
   ```
   This creates a `hf_export/` directory with `model.safetensors`, `config.json`, and the necessary `.py` files.

2. **Publish to Hub**:
   Use the Hugging Face CLI to upload the export bundle:
   ```bash
   huggingface-cli upload kengurukleo/deutsch_a2_transformer ./hf_export .
   ```
   Or push to `main` and let the GitHub Action do it (see below).

### Pre-commit: auto-export HF model on every commit
To regenerate `hf_export/` automatically before each commit (so the bundle always matches `model_final.pth`):

```bash
pip install pre-commit
pre-commit install
```

After that, every `git commit` will run `python src/export_hf.py` and stage `hf_export/`. If `model_final.pth` is missing, the hook skips without blocking the commit.

### GitHub Action: push model to Hugging Face
The workflow [`.github/workflows/push-to-hf.yml`](.github/workflows/push-to-hf.yml) pushes `hf_export/` to [kengurukleo/deutsch_a2_transformer](https://huggingface.co/kengurukleo/deutsch_a2_transformer) when you push to `main` (if relevant files changed) or when you run it manually (**Actions â†’ Push model to Hugging Face â†’ Run workflow**).

**Required:** Add a repository secret with a Hugging Face write token:
- **Settings â†’ Secrets and variables â†’ Actions â†’ New repository secret**
- Name: `HF_TOKEN` or `HG_TOKEN`
- Value: your token from [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens) (with write access)

The workflow expects `model_final.pth` and `config.yaml` to be in the repo (e.g. committed after training).

## Data Format (JSONL)

```json
{
  "input": "Heute ich gehe ins Kino.",
  "output": "âŒ Incorrect.\nâœ… Correct: Heute gehe ich ins Kino.\nğŸ“ Explanation: The verb must be in the second position."
}
```

## License

MIT
