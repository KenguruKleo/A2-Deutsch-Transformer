# ğŸ‡©ğŸ‡ª A2 Deutsch Grammar Tutor

A compact Transformer model for checking and explaining errors in German sentences at the A2 level. 
Trained to correct grammar and provide simple explanations **in Ukrainian**.

ğŸ¤— **Model on Hugging Face:** [kengurukleo/deutsch_a2_transformer](https://huggingface.co/kengurukleo/deutsch_a2_transformer)  
âœ¨ **Live Demo (Space):** [kengurukleo/deutsch-a2-tutor](https://huggingface.co/spaces/kengurukleo/deutsch-a2-tutor)

## Features

| Function | Example |
|---|---|
| âœ… Validity Check | `Ich bin nach Hause gegangen.` â†’ âœ… |
| âŒ Error Correction | `Dann ich gehe...` â†’ `Dann gehe ich...` |
| ğŸ“ Explanations | Detailed grammar feedback provided in **Ukrainian**. |

### âœ… Covered A1-A2 Topics (100% Complete)

| Topic | Level | What the model does / Explanation | Status |
|:---|:---:|:---|:---:|
| **PrÃ¤sens** | A1 | Verb conjugation (e.g., *ich esse, du isst*). | âœ… |
| **W-Fragen** | A1 | Question word order (*Wo wohnst du?*). | âœ… |
| **Akkusativ** | A1 | Masculine article change (*der -> den*). | âœ… |
| **Negation** | A1 | Usage of *nicht* vs. *kein* (noun vs. adj/verb). | âœ… |
| **Modalverben** | A1/A2 | Conjugation and putting the main verb at the end. | âœ… |
| **Possessivpron.**| A1/A2 | Agreement of *mein, dein, sein...* in Nominativ. | âœ… |
| **Fixed Prepos.** | A1/A2 | Prepositions with fixed cases (*mit + Dat*, *fÃ¼r + Akk*). | âœ… |
| **Imperativ** | A1/A2 | Command forms for *du*, *ihr*, and *Sie*. | âœ… |
| **Perfekt** | A2 | Choosing *haben* vs. *sein* and *Partizip II* forms. | âœ… |
| **Inversion** | A2 | Verb-second rule after adverbs (*Heute gehe ich...*). | âœ… |
| **Separable Verbs**| A2 | Prefix position in present tense (*Ich kaufe ein*). | âœ… |
| **Dativ** | A2 | Articles after Dativ-governing verbs (*helfen, danken*). | âœ… |
| **WechselprÃ¤p.** | A2 | Two-way prepositions (Wohin? -> Akk / Wo? -> Dat). | âœ… |
| **NebensÃ¤tze** | A2 | Verb-last order in clauses with *weil, dass, wenn*. | âœ… |
| **Adjektivdekl.** | A2 | Basic endings after *ein* in Nominativ. | âœ… |
| **Reflexive Verben**| A2 | Correct reflexive pronouns (*mich, dich, sich...*). | âœ… |
| **PrÃ¤teritum** | A2 | Past tense forms for *sein* and *haben* (*war, hatte*). | âœ… |
| **Komparation** | A2 | Adjective comparison (*gut - besser*, not *mehr gut*). | âœ… |

For a complete list of examples and model explanations for each topic, see:  
ğŸ‘‰ **[Grammar Topics & Examples](docs/topics_examples.md)**

## Architecture

```
Transformer Decoder Only
â”œâ”€â”€ V = 4,000 tokens (words + forms + explanations)
â”œâ”€â”€ T = 64  (max sequence length)
â”œâ”€â”€ d_model = 128
â”œâ”€â”€ L = 4 Layers
â”œâ”€â”€ H = 4 Attention Heads
â”œâ”€â”€ Weight tying = ON (shared weights between Embeddings and LM Head)
â””â”€â”€ Precision = FP16 â†’ Model size â‰ˆ 2.5 MB

Detailed mathematical description of all matrix transformations can be found in [docs/architecture.md](docs/architecture.md).
```

## Project Structure

```text
A2-Deutsch-Transformer/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ model.py                # Core Transformer architecture
â”‚   â”‚   â”œâ”€â”€ configuration_custom.py # HF Config wrapper
â”‚   â”‚   â””â”€â”€ modeling_custom.py      # HF Model wrapper (custom code)
â”‚   â”œâ”€â”€ tokenizer/
â”‚   â”‚   â”œâ”€â”€ build_vocab.py          # PDF analysis and vocab creation
â”‚   â”‚   â””â”€â”€ tokenizer.py            # Word-level tokenizer
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ generator.py            # Main synthetic data generator
â”‚   â”‚   â””â”€â”€ generators/             # Specialized topic generators
â”‚   â”œâ”€â”€ train.py                    # Training loop (MPS optimized)
â”‚   â”œâ”€â”€ generate.py                 # CLI inference script
â”‚   â””â”€â”€ export_hf.py                # Hugging Face export script
â”œâ”€â”€ hf_export/                      # Bundle for HF Hub (weights + code)
â”œâ”€â”€ hf_space/                       # Bundle for HF Spaces (Gradio app)
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_model.py               # Architecture and device tests
â”œâ”€â”€ data/                           # Generated JSONL datasets
â”œâ”€â”€ data_raw/                       # Raw PDF textbooks
â”œâ”€â”€ docs/                           # Architecture and grammar docs
â”œâ”€â”€ config.yaml                     # Model & training hyperparameters
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## How It Works

### 1. `build_vocab.py`
Creates the "brain" of the tokenizer. It analyzes the `Begegnungen_A2.pdf` textbook, extracts the most frequent German words, adds conjugation tables, and includes words for explanations. The result is a `vocab.json` file with 4000 unique tokens.

### 2. `generator.py`
Generates thousands of training examples. It knows grammar rules, takes a correct sentence and intentionally "breaks" it (e.g., changes word order or auxiliary verb), adding an explanation of why it is an error.

### 3. Training
The model is trained locally on **Apple Silicon (M1/M2/M3)** using `torch.device("mps")`. Due to its small size (2.5 MB), training takes only a few minutes.

## Installation & Setup

Follow these steps to initialize the project and set up the environment:

```bash
# 1. Clone the repository (if not already done)
git clone https://github.com/KenguruKleo/A2-Deutsch-Transformer.git
cd A2-Deutsch-Transformer

# 2. Create a virtual environment
python3 -m venv .venv

# 3. Activate the virtual environment
source .venv/bin/activate  # On macOS/Linux
# .venv\Scripts\activate     # On Windows

# 4. Install dependencies
pip install --upgrade pip
pip install -r requirements.txt
```

## Testing

To verify the model architecture and device compatibility, run the following command:

```bash
# Run model unit tests
python tests/test_model.py
```

These tests check:
- Output dimensions (`[batch, seq_len, vocab_size]`).
- Successful execution on **MPS** (Apple Silicon) or **CPU**.

## Quick Start

Once the environment is set up and activated:

```bash
# 1. Build vocabulary (if changing word lists)
python src/tokenizer/build_vocab.py

# 2. Generate training data
python src/data/generator.py

# 3. Run training
python src/train.py

# 4. Test the model
python src/generate.py --text "Ich habe nach Berlin gefahren."

# 5. Export to Hugging Face format
python src/export_hf.py
```

## Hugging Face Hub

This model is hosted on the [Hugging Face Hub](https://huggingface.co/kengurukleo/deutsch_a2_transformer). Below are instructions for users and developers.

### ğŸ“¥ For Users (Loading the Model)
You can load and use this model directly in your Python code using the `transformers` library. Note that `trust_remote_code=True` is required because the model uses a custom architecture and tokenizer.

```python
from transformers import AutoModelForCausalLM

# Load model and use custom code from the Hub
model = AutoModelForCausalLM.from_pretrained(
    "kengurukleo/deutsch_a2_transformer", 
    trust_remote_code=True
)
```

### ğŸ›  For Developers (Export and Publish)
If you want to re-export the model or publish your own version:

1. **Export to Safetensors**:
   Run the export script to create a compatible bundle (weights, config, and source code):
   ```bash
   python src/export_hf.py
   ```
   This creates a `hf_export/` directory with `model.safetensors`, `config.json`, and the necessary `.py` files.

2. **Publish to Hub**:
   Use the Hugging Face CLI to upload the export bundle:
   ```bash
   huggingface-cli upload kengurukleo/deutsch_a2_transformer ./hf_export .
   ```

## Data Format (JSONL)

```json
{
  "input": "Heute ich gehe ins Kino.",
  "output": "âŒ Incorrect.\nâœ… Correct: Heute gehe ich ins Kino.\nğŸ“ Explanation: The verb must be in the second position."
}
```

## License

MIT
